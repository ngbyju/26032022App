{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b71b0d5",
   "metadata": {},
   "source": [
    "# Using Learning Rate Schedules for Deep Learning Models in Python with Keras\n",
    "https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13464f48",
   "metadata": {},
   "source": [
    "Training a neural network or large deep learning model is a difficult optimization task.\n",
    "\n",
    "The classical algorithm to train neural networks is called stochastic gradient descent. It has been well established that you can achieve increased performance and faster training on some problems by using a learning rate that changes during training.\n",
    "\n",
    "In this post, you will discover how you can use different learning rate schedules for your neural network models in Python using the Keras deep learning library.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "How to configure and evaluate a time-based learning rate schedule\n",
    "How to configure and evaluate a drop-based learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814460c",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule for Training Models\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639312ae",
   "metadata": {},
   "source": [
    "Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time.\n",
    "\n",
    "Sometimes, this is called learning rate annealing or adaptive learning rates. Here, this approach is called a learning rate schedule, where the default schedule uses a constant learning rate to update network weights for each training epoch.\n",
    "\n",
    "The simplest and perhaps most used adaptation of the learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used and decreasing the learning rate so that a smaller rate and, therefore, smaller training updates are made to weights later in the training procedure.\n",
    "\n",
    "This has the effect of quickly learning good weights early and fine-tuning them later.\n",
    "\n",
    "Two popular and easy-to-use learning rate schedules are as follows:\n",
    "\n",
    "Decrease the learning rate gradually based on the epoch\n",
    "Decrease the learning rate using punctuated large drops at specific epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830f581",
   "metadata": {},
   "source": [
    "# Time-Based Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "raw",
   "id": "456caed5",
   "metadata": {},
   "source": [
    "\n",
    "Keras has a built-in time-based learning rate schedule.\n",
    "\n",
    "The stochastic gradient descent optimization algorithm implementation in the SGD class has an argument called decay. This argument is used in the time-based learning rate decay schedule equation as follows:\n",
    "\n",
    "LearningRate = LearningRate * 1/(1 + decay * epoch)\n",
    "When the decay argument is zero (the default), this does not affect the learning rate.\n",
    "\n",
    "LearningRate = 0.1 * 1/(1 + 0.0 * 1)\n",
    "LearningRate = 0.1\n",
    "When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
    "\n",
    "For example, if you use the initial learning rate value of 0.1 and the decay of 0.001, the first five epochs will adapt the learning rate as follows:\n",
    "\n",
    "Epoch\tLearning Rate\n",
    "1\t0.1\n",
    "2\t0.0999000999\n",
    "3\t0.0997006985\n",
    "4\t0.09940249103\n",
    "5\t0.09900646517\n",
    "Extending this out to 100 epochs will produce the following graph of learning rate (y-axis) versus epoch (x-axis):\n",
    "\n",
    "Time-Based Learning Rate Schedule\n",
    "Time-based learning rate schedule\n",
    "\n",
    "You can create a nice default schedule by setting the decay value as follows:\n",
    "\n",
    "Decay = LearningRate / Epochs\n",
    "Decay = 0.1 / 100\n",
    "Decay = 0.001\n",
    "The example below demonstrates using the time-based learning rate adaptation schedule in Keras.\n",
    "\n",
    "It is demonstrated in the Ionosphere binary classification problem. This is a small dataset that you can download from the UCI Machine Learning repository. Place the data file in your working directory with the filename ionosphere.csv.\n",
    "\n",
    "The ionosphere dataset is good for practicing with neural networks because all the input values are small numerical values of the same scale.\n",
    "\n",
    "A small neural network model is constructed with a single hidden layer with 34 neurons, using the rectifier activation function. The output layer has a single neuron and uses the sigmoid activation function in order to output probability-like values.\n",
    "\n",
    "The learning rate for stochastic gradient descent has been set to a higher value of 0.1. The model is trained for 50 epochs, and the decay argument has been set to 0.002, calculated as 0.1/50. Additionally, it can be a good idea to use momentum when using an adaptive learning rate. In this case, we use a momentum value of 0.8.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac462c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 0s - loss: 0.6954 - accuracy: 0.5574 - val_loss: 0.3888 - val_accuracy: 0.8793 - 412ms/epoch - 46ms/step\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.4870 - accuracy: 0.8085 - val_loss: 0.3555 - val_accuracy: 0.9483 - 23ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.3440 - accuracy: 0.8809 - val_loss: 0.4456 - val_accuracy: 0.8621 - 23ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.2956 - accuracy: 0.9191 - val_loss: 0.1817 - val_accuracy: 0.9569 - 23ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.2115 - accuracy: 0.9362 - val_loss: 0.4100 - val_accuracy: 0.8276 - 25ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.2097 - accuracy: 0.9404 - val_loss: 0.1359 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1754 - accuracy: 0.9404 - val_loss: 0.1573 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1500 - accuracy: 0.9489 - val_loss: 0.1100 - val_accuracy: 0.9828 - 25ms/epoch - 3ms/step\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1490 - accuracy: 0.9489 - val_loss: 0.1275 - val_accuracy: 0.9828 - 23ms/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.1261 - accuracy: 0.9745 - val_loss: 0.1338 - val_accuracy: 0.9828 - 30ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.1174 - accuracy: 0.9745 - val_loss: 0.1511 - val_accuracy: 0.9828 - 28ms/epoch - 3ms/step\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.1119 - accuracy: 0.9702 - val_loss: 0.0931 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.1044 - accuracy: 0.9787 - val_loss: 0.1438 - val_accuracy: 0.9828 - 24ms/epoch - 3ms/step\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.1022 - accuracy: 0.9830 - val_loss: 0.0989 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.0932 - accuracy: 0.9745 - val_loss: 0.1012 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.0939 - accuracy: 0.9745 - val_loss: 0.1449 - val_accuracy: 0.9828 - 23ms/epoch - 3ms/step\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.0866 - accuracy: 0.9830 - val_loss: 0.0874 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.0814 - accuracy: 0.9830 - val_loss: 0.1399 - val_accuracy: 0.9828 - 24ms/epoch - 3ms/step\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0841 - accuracy: 0.9830 - val_loss: 0.0912 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0781 - accuracy: 0.9830 - val_loss: 0.0806 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0776 - accuracy: 0.9830 - val_loss: 0.1164 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0745 - accuracy: 0.9830 - val_loss: 0.0900 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0714 - accuracy: 0.9830 - val_loss: 0.0740 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0753 - accuracy: 0.9787 - val_loss: 0.1001 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0643 - accuracy: 0.9830 - val_loss: 0.0762 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0643 - accuracy: 0.9830 - val_loss: 0.0870 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0613 - accuracy: 0.9830 - val_loss: 0.0784 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0590 - accuracy: 0.9830 - val_loss: 0.0787 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0574 - accuracy: 0.9830 - val_loss: 0.0779 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0588 - accuracy: 0.9830 - val_loss: 0.0924 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0551 - accuracy: 0.9872 - val_loss: 0.0696 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0581 - accuracy: 0.9830 - val_loss: 0.0857 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0527 - accuracy: 0.9872 - val_loss: 0.0718 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0517 - accuracy: 0.9872 - val_loss: 0.0824 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0509 - accuracy: 0.9872 - val_loss: 0.0692 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0619 - accuracy: 0.9830 - val_loss: 0.0883 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0524 - accuracy: 0.9915 - val_loss: 0.0623 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0519 - accuracy: 0.9872 - val_loss: 0.0831 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0476 - accuracy: 0.9915 - val_loss: 0.0691 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0495 - accuracy: 0.9872 - val_loss: 0.0727 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0488 - accuracy: 0.9915 - val_loss: 0.0659 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0465 - accuracy: 0.9872 - val_loss: 0.0663 - val_accuracy: 0.9741 - 25ms/epoch - 3ms/step\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0456 - accuracy: 0.9872 - val_loss: 0.0701 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0782 - val_accuracy: 0.9828 - 24ms/epoch - 3ms/step\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0422 - accuracy: 0.9915 - val_loss: 0.0686 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.0707 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0427 - accuracy: 0.9915 - val_loss: 0.0726 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0404 - accuracy: 0.9915 - val_loss: 0.0710 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.0719 - val_accuracy: 0.9741 - 23ms/epoch - 3ms/step\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0428 - accuracy: 0.9915 - val_loss: 0.0696 - val_accuracy: 0.9741 - 24ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232d2d96a30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "dataframe = read_csv(\"ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_shape=(34,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9abb2872",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "The model is trained on 67% of the dataset and evaluated using a 33% validation dataset.\n",
    "\n",
    "Running the example shows a classification accuracy of 99.14%. This is higher than the baseline of 95.69% without the learning rate decay or momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d28123",
   "metadata": {},
   "source": [
    "# Drop-Based Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f773d912",
   "metadata": {},
   "source": [
    "\n",
    "Another popular learning rate schedule used with deep learning models is systematically dropping the learning rate at specific times during training.\n",
    "\n",
    "Often this method is implemented by dropping the learning rate by half every fixed number of epochs. For example, we may have an initial learning rate of 0.1 and drop it by 0.5 every ten epochs. The first ten epochs of training would use a value of 0.1, and in the next ten epochs, a learning rate of 0.05 would be used, and so on.\n",
    "\n",
    "If you plot the learning rates for this example out to 100 epochs, you get the graph below showing the learning rate (y-axis) versus epoch (x-axis).\n",
    "\n",
    "Drop Based Learning Rate Schedule\n",
    "Drop-based learning rate schedule\n",
    "\n",
    "You can implement this in Keras using the LearningRateScheduler callback when fitting the model.\n",
    "\n",
    "The LearningRateScheduler callback allows you to define a function to call that takes the epoch number as an argument and returns the learning rate to use in stochastic gradient descent. When used, the learning rate specified by stochastic gradient descent is ignored.\n",
    "\n",
    "In the code below, we use the same example as before of a single hidden layer network on the Ionosphere dataset. A new step_decay() function is defined that implements the equation:\n",
    "\n",
    "LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)\n",
    "Here, the InitialLearningRate is the initial learning rate (such as 0.1), the DropRate is the amount that the learning rate is modified each time it is changed (such as 0.5), Epoch is the current epoch number, and EpochDrop is how often to change the learning rate (such as 10).\n",
    "\n",
    "Notice that the learning rate in the SGD class is set to 0 to clearly indicate that it is not used. Nevertheless, you can set a momentum term in SGD if you want to use momentum with this learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959ed841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 0s - loss: 0.5605 - accuracy: 0.7404 - val_loss: 0.3821 - val_accuracy: 0.9224 - lr: 0.1000 - 359ms/epoch - 40ms/step\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.3742 - accuracy: 0.8766 - val_loss: 0.1948 - val_accuracy: 0.9741 - lr: 0.1000 - 33ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.2739 - accuracy: 0.8979 - val_loss: 0.4707 - val_accuracy: 0.7586 - lr: 0.1000 - 28ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.2742 - accuracy: 0.9064 - val_loss: 0.2209 - val_accuracy: 0.9741 - lr: 0.1000 - 29ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.1851 - accuracy: 0.9319 - val_loss: 0.1562 - val_accuracy: 0.9828 - lr: 0.1000 - 20ms/epoch - 2ms/step\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.1454 - accuracy: 0.9574 - val_loss: 0.1324 - val_accuracy: 0.9741 - lr: 0.1000 - 31ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1326 - accuracy: 0.9574 - val_loss: 0.0652 - val_accuracy: 0.9741 - lr: 0.1000 - 24ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1327 - accuracy: 0.9532 - val_loss: 0.4039 - val_accuracy: 0.7845 - lr: 0.1000 - 20ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1362 - accuracy: 0.9532 - val_loss: 0.0647 - val_accuracy: 0.9828 - lr: 0.1000 - 28ms/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.1117 - accuracy: 0.9660 - val_loss: 0.1506 - val_accuracy: 0.9569 - lr: 0.0500 - 28ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.0996 - accuracy: 0.9702 - val_loss: 0.0703 - val_accuracy: 0.9914 - lr: 0.0500 - 21ms/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.0900 - accuracy: 0.9702 - val_loss: 0.0857 - val_accuracy: 0.9914 - lr: 0.0500 - 28ms/epoch - 3ms/step\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.0891 - accuracy: 0.9745 - val_loss: 0.0917 - val_accuracy: 0.9914 - lr: 0.0500 - 28ms/epoch - 3ms/step\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.0755 - accuracy: 0.9830 - val_loss: 0.0730 - val_accuracy: 0.9914 - lr: 0.0500 - 28ms/epoch - 3ms/step\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.0728 - accuracy: 0.9787 - val_loss: 0.0652 - val_accuracy: 0.9914 - lr: 0.0500 - 27ms/epoch - 3ms/step\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.0722 - accuracy: 0.9787 - val_loss: 0.0687 - val_accuracy: 0.9914 - lr: 0.0500 - 21ms/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.0698 - accuracy: 0.9787 - val_loss: 0.0860 - val_accuracy: 0.9914 - lr: 0.0500 - 28ms/epoch - 3ms/step\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.0650 - accuracy: 0.9830 - val_loss: 0.0625 - val_accuracy: 0.9914 - lr: 0.0500 - 31ms/epoch - 3ms/step\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0633 - accuracy: 0.9830 - val_loss: 0.0704 - val_accuracy: 0.9914 - lr: 0.0500 - 23ms/epoch - 3ms/step\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0625 - accuracy: 0.9830 - val_loss: 0.0759 - val_accuracy: 0.9914 - lr: 0.0250 - 26ms/epoch - 3ms/step\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0592 - accuracy: 0.9830 - val_loss: 0.0657 - val_accuracy: 0.9914 - lr: 0.0250 - 21ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0583 - accuracy: 0.9830 - val_loss: 0.0625 - val_accuracy: 0.9914 - lr: 0.0250 - 28ms/epoch - 3ms/step\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0575 - accuracy: 0.9830 - val_loss: 0.0622 - val_accuracy: 0.9914 - lr: 0.0250 - 27ms/epoch - 3ms/step\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0573 - accuracy: 0.9830 - val_loss: 0.0671 - val_accuracy: 0.9914 - lr: 0.0250 - 25ms/epoch - 3ms/step\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0567 - accuracy: 0.9830 - val_loss: 0.0606 - val_accuracy: 0.9914 - lr: 0.0250 - 27ms/epoch - 3ms/step\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0561 - accuracy: 0.9830 - val_loss: 0.0694 - val_accuracy: 0.9914 - lr: 0.0250 - 25ms/epoch - 3ms/step\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0549 - accuracy: 0.9830 - val_loss: 0.0632 - val_accuracy: 0.9914 - lr: 0.0250 - 25ms/epoch - 3ms/step\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0539 - accuracy: 0.9830 - val_loss: 0.0625 - val_accuracy: 0.9914 - lr: 0.0250 - 25ms/epoch - 3ms/step\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0528 - accuracy: 0.9872 - val_loss: 0.0679 - val_accuracy: 0.9914 - lr: 0.0250 - 27ms/epoch - 3ms/step\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0530 - accuracy: 0.9872 - val_loss: 0.0700 - val_accuracy: 0.9914 - lr: 0.0125 - 25ms/epoch - 3ms/step\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0524 - accuracy: 0.9872 - val_loss: 0.0641 - val_accuracy: 0.9914 - lr: 0.0125 - 25ms/epoch - 3ms/step\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0516 - accuracy: 0.9872 - val_loss: 0.0607 - val_accuracy: 0.9914 - lr: 0.0125 - 27ms/epoch - 3ms/step\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0509 - accuracy: 0.9872 - val_loss: 0.0587 - val_accuracy: 0.9914 - lr: 0.0125 - 24ms/epoch - 3ms/step\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0509 - accuracy: 0.9872 - val_loss: 0.0598 - val_accuracy: 0.9914 - lr: 0.0125 - 21ms/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0505 - accuracy: 0.9872 - val_loss: 0.0604 - val_accuracy: 0.9914 - lr: 0.0125 - 29ms/epoch - 3ms/step\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0502 - accuracy: 0.9872 - val_loss: 0.0597 - val_accuracy: 0.9914 - lr: 0.0125 - 27ms/epoch - 3ms/step\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0499 - accuracy: 0.9872 - val_loss: 0.0582 - val_accuracy: 0.9914 - lr: 0.0125 - 28ms/epoch - 3ms/step\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0497 - accuracy: 0.9872 - val_loss: 0.0583 - val_accuracy: 0.9914 - lr: 0.0125 - 28ms/epoch - 3ms/step\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0490 - accuracy: 0.9872 - val_loss: 0.0618 - val_accuracy: 0.9914 - lr: 0.0125 - 28ms/epoch - 3ms/step\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0490 - accuracy: 0.9872 - val_loss: 0.0618 - val_accuracy: 0.9914 - lr: 0.0063 - 28ms/epoch - 3ms/step\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0491 - accuracy: 0.9872 - val_loss: 0.0603 - val_accuracy: 0.9914 - lr: 0.0063 - 20ms/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0487 - accuracy: 0.9915 - val_loss: 0.0607 - val_accuracy: 0.9914 - lr: 0.0063 - 28ms/epoch - 3ms/step\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0488 - accuracy: 0.9915 - val_loss: 0.0586 - val_accuracy: 0.9914 - lr: 0.0063 - 28ms/epoch - 3ms/step\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0484 - accuracy: 0.9915 - val_loss: 0.0581 - val_accuracy: 0.9914 - lr: 0.0063 - 30ms/epoch - 3ms/step\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0482 - accuracy: 0.9915 - val_loss: 0.0590 - val_accuracy: 0.9914 - lr: 0.0063 - 27ms/epoch - 3ms/step\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0482 - accuracy: 0.9915 - val_loss: 0.0594 - val_accuracy: 0.9914 - lr: 0.0063 - 26ms/epoch - 3ms/step\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0481 - accuracy: 0.9915 - val_loss: 0.0578 - val_accuracy: 0.9914 - lr: 0.0063 - 24ms/epoch - 3ms/step\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0478 - accuracy: 0.9915 - val_loss: 0.0593 - val_accuracy: 0.9914 - lr: 0.0063 - 25ms/epoch - 3ms/step\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0478 - accuracy: 0.9915 - val_loss: 0.0595 - val_accuracy: 0.9914 - lr: 0.0063 - 25ms/epoch - 3ms/step\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0475 - accuracy: 0.9915 - val_loss: 0.0580 - val_accuracy: 0.9914 - lr: 0.0031 - 25ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232db333490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Drop-Based Learning Rate Decay\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    " \n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.1\n",
    "\tdrop = 0.5\n",
    "\tepochs_drop = 10.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lrate\n",
    " \n",
    "# load dataset\n",
    "dataframe = read_csv(\"ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_shape=(34,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "sgd = SGD(learning_rate=0.0, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70b0ee43",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example results in a classification accuracy of 99.14% on the validation dataset, again an improvement over the baseline for the model of the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a773d1",
   "metadata": {},
   "source": [
    "# Tips for Using Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0bba64f",
   "metadata": {},
   "source": [
    "\n",
    "This section lists some tips and tricks to consider when using learning rate schedules with neural networks.\n",
    "\n",
    "1) Increase the initial learning rate.\n",
    "\n",
    "Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine-tuning later.\n",
    "\n",
    "2) Use a large momentum.\n",
    "\n",
    "Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values.\n",
    "\n",
    "3) Experiment with different schedules. \n",
    "\n",
    "It will not be clear which learning rate schedule to use, so try a few with different configuration options and see what works best on your problem. Also, try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a116f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
