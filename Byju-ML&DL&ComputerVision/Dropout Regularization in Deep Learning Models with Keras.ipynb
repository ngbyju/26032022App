{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a18c3fc",
   "metadata": {},
   "source": [
    "# Dropout Regularization in Deep Learning Models with Keras\n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout is a simple and powerful regularization technique for neural networks and deep learning models.\n",
    "\n",
    "In this post, you will discover the Dropout regularization technique and how to apply it to your models in Python with Keras.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "How the Dropout regularization technique works\n",
    "How to use Dropout on your input layers\n",
    "How to use Dropout on your hidden layers\n",
    "How to tune the dropout level on your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828e779",
   "metadata": {},
   "source": [
    "# Dropout Regularization for Neural Networks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0d6f67b",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique for neural network models proposed by Srivastava et al. in their 2014 paper “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (download the PDF).\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features, providing some specialization. Neighboring neurons come to rely on this specialization, which, if taken too far, can result in a fragile model too specialized for the training data. This reliance on context for a neuron during training is referred to as complex co-adaptations.\n",
    "\n",
    "You can imagine that if neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n",
    "\n",
    "The effect is that the network becomes less sensitive to the specific weights of neurons. This, in turn, results in a network capable of better generalization and less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60398c4e",
   "metadata": {},
   "source": [
    "# Dropout Regularization in Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb6bdefd",
   "metadata": {},
   "source": [
    "\n",
    "Dropout is easily implemented by randomly selecting nodes to be dropped out with a given probability (e.g., 20%) in each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model.\n",
    "\n",
    "Next, let’s explore a few different ways of using Dropout in Keras.\n",
    "\n",
    "The examples will use the Sonar dataset. This is a binary classification problem that aims to correctly identify rocks and mock-mines from sonar chirp returns. It is a good test dataset for neural networks because all the input values are numerical and have the same scale.\n",
    "\n",
    "The dataset can be downloaded from the UCI Machine Learning repository. You can place the sonar dataset in your current working directory with the file name sonar.csv.\n",
    "\n",
    "You will evaluate the developed models using scikit-learn with 10-fold cross validation in order to tease out differences in the results better.\n",
    "\n",
    "There are 60 input values and a single output value. The input values are standardized before being used in the network. The baseline neural network model has two hidden layers, the first with 60 units and the second with 30. Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum.\n",
    "\n",
    "The full baseline model is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25d4375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 81.79% (8.65%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baseline Model on the Sonar Dataset\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    " \n",
    "# baseline\n",
    "def create_baseline():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "\tmodel.add(Dense(30,  activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(learning_rate=0.01, momentum=0.8)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "152d9fec",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example generates an estimated classification accuracy of 86%.\n",
    "\n",
    "Baseline: 86.04% (4.58%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122626c6",
   "metadata": {},
   "source": [
    "# Using Dropout on the Visible Layer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e08d8a8a",
   "metadata": {},
   "source": [
    "\n",
    "Dropout can be applied to input neurons called the visible layer.\n",
    "\n",
    "In the example below,  a new Dropout layer between the input (or visible layer) and the first hidden layer was added. The dropout rate is set to 20%, meaning one in five inputs will be randomly excluded from each update cycle.\n",
    "\n",
    "Additionally, as recommended in the original paper on Dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the kernel_constraint argument on the Dense class when constructing the layers.\n",
    "\n",
    "The learning rate was lifted by one order of magnitude, and the momentum was increased to 0.9. These increases in the learning rate were also recommended in the original Dropout paper.\n",
    "\n",
    "Continuing from the baseline example above, the code below exercises the same network with input dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfff10b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 85.98% (7.14%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    " \n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dropout(0.2, input_shape=(60,)))\n",
    "\tmodel.add(Dense(60, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "\tmodel.add(Dense(30, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(learning_rate=0.1, momentum=0.9)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_model, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c475ba9",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example provides a slight drop in classification accuracy, at least on a single test run.\n",
    "\n",
    "Visible: 83.52% (7.68%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100b8a2",
   "metadata": {},
   "source": [
    "# Using Dropout on Hidden Layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "787df28f",
   "metadata": {},
   "source": [
    "\n",
    "Dropout can be applied to hidden neurons in the body of your network model.\n",
    "\n",
    "In the example below, Dropout is applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout rate of 20% is used as is a weight constraint on those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of Dropout on the Sonar Dataset: Hidden Layer\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    " \n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_shape=(60,), activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(learning_rate=0.1, momentum=0.9)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_model, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e36ca547",
   "metadata": {},
   "source": [
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "You can see that for this problem and the chosen network configuration, using Dropout in the hidden layers did not lift performance. In fact, performance was worse than the baseline.\n",
    "\n",
    "It is possible that additional training epochs are required or that further tuning is required to the learning rate.\n",
    "\n",
    "Hidden: 83.59% (7.31%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620a226",
   "metadata": {},
   "source": [
    "# Dropout in Evaluation Mode"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffc50f3b",
   "metadata": {},
   "source": [
    "\n",
    "Dropout will randomly reset some of the input to zero. If you wonder what happens after you have finished training, the answer is nothing! In Keras, a layer can tell if the model is running in training mode or not. The Dropout layer will randomly reset some input only when the model runs for training. Otherwise, the Dropout layer works as a scaler to multiply all input by a factor such that the next layer will see input similar in scale. Precisely, if the dropout rate is , the input will be scaled by a factor of .\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b9e0ad8",
   "metadata": {},
   "source": [
    "# Tips for Using Dropout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9f33dc1",
   "metadata": {},
   "source": [
    "\n",
    "The original paper on Dropout provides experimental results on a suite of standard machine learning problems. As a result, they provide a number of useful heuristics to consider when using Dropout in practice.\n",
    "\n",
    "Generally, use a small dropout value of 20%-50% of neurons, with 20% providing a good starting point. A probability too low has minimal effect, and a value too high results in under-learning by the network.\n",
    "Use a larger network. You are likely to get better performance when Dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "Use Dropout on incoming (visible) as well as hidden units. Application of Dropout at each layer of the network has shown good results.\n",
    "Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights, such as max-norm regularization, with a size of 4 or 5 has been shown to improve results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "df2114d4",
   "metadata": {},
   "source": [
    "\n",
    "More Resources on Dropout\n",
    "Below are resources you can use to learn more about Dropout in neural networks and deep learning models.\n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "Dropout: A Simple Way to Prevent Neural Networks from Overfitting (original paper)\n",
    "Improving neural networks by preventing co-adaptation of feature detectors\n",
    "How does the dropout method work in deep learning? on Quora\n",
    "Keras Training and Evaluation with Built-in Methods from TensorFlow documentation\n",
    "Summary\n",
    "In this post, you discovered the Dropout regularization technique for deep learning models. You learned:\n",
    "\n",
    "What Dropout is and how it works\n",
    "How you can use Dropout on your own deep learning models.\n",
    "Tips for getting the best results from Dropout on your own models.\n",
    "Do you have any questions about Dropout or this post? Ask your questions in the comments, and I will do my best to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce7da8",
   "metadata": {},
   "source": [
    "# https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
