{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdefe71",
   "metadata": {},
   "source": [
    "# Project: Sequence Classification of IMDB Movie Reviews\n",
    "Deep Learning with Python: Jason Brownlee, Page 211-194 of 255\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79774bcd",
   "metadata": {},
   "source": [
    "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long term context or dependencies\n",
    "between symbols in the input sequence. In this project you will discover how you can develop LSTM recurrent neural network models for sequence classification problems in Python using the Keras deep learning library. After completing this project you will know: \n",
    "\n",
    "How to develop an LSTM model for a sequence classification problem.\n",
    "How to reduce overfitting in your LSTM models through the use of dropout.\n",
    "How to combine LSTM models with Convolutional Neural Networks that excel at learning spatial relationships.\n",
    "\n",
    "Let’s get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c51168",
   "metadata": {},
   "source": [
    "# Simple LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54683282",
   "metadata": {},
   "source": [
    "The problem that we will use to demonstrate sequence learning in this tutorial is the IMDB movie review sentiment  classification problem, introduced in Section 22.1. We can quickly develop a small LSTM for the IMDB problem and achieve good accuracy. Let’s start o↵ by importing the classes and functions required for this model and initializing the random number\n",
    "generator to a constant value to ensure we can easily reproduce the results.\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets. \n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras.\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "We can now define, compile and fit our LSTM model. The first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem. Because it is a binary classification problem, log loss is used as the loss function (binary crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates.\n",
    "\n",
    "# create the model \n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation= sigmoid ))\n",
    "model.compile(loss= binary_crossentropy , optimizer= adam , metrics=[ accuracy ])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "\n",
    "Once fit, we estimate the performance of the model on unseen reviews.\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "For completeness, here is the full code listing for this LSTM network on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a6d993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " ...\n",
      " list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2])\n",
      " list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23])\n",
      " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n",
      "\n",
      "\n",
      "print again\n",
      "\n",
      "\n",
      "[[   0    0    0 ...   19  178   32]\n",
      " [   0    0    0 ...   16  145   95]\n",
      " [   0    0    0 ...    7  129  113]\n",
      " ...\n",
      " [   0    0    0 ...    4 3586    2]\n",
      " [   0    0    0 ...   12    9   23]\n",
      " [   0    0    0 ...  204  131    9]]\n"
     ]
    }
   ],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Lets load and print data to appreciate the discrete text data converted to continuous numeric vector for embedding\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print(X_train)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print(\"\\n\\nprint again\\n\\n\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26290f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 286s 726ms/step - loss: 0.4466 - accuracy: 0.7807\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 304s 776ms/step - loss: 0.2866 - accuracy: 0.8881\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 252s 645ms/step - loss: 0.2466 - accuracy: 0.9049\n",
      "Accuracy: 87.20%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:15:06 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation= \"sigmoid\" ))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[ \"accuracy\" ])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ea65566",
   "metadata": {},
   "source": [
    "You can see that this simple LSTM with little tuning achieves near state-of-the-art results on the IMDB problem. Importantly, this is a template that you can use to apply LSTM networks to your own sequence classification problems. Now, let’s look at some extensions of this simple model that you may also want to bring to your own problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0e747",
   "metadata": {},
   "source": [
    "# LSTM For Sequence Classification With Dropout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6722b20",
   "metadata": {},
   "source": [
    "Recurrent Neural networks like LSTM generally have the problem of overfitting. Dropout can be applied between layers using the Dropout Keras layer. We can do this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8b3e011",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation= sigmoid ))\n",
    "\n",
    "The full code listing example above with the addition of Dropout layers is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc72b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 272s 691ms/step - loss: 0.4611 - accuracy: 0.7794\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 270s 692ms/step - loss: 0.3037 - accuracy: 0.8774\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 252s 645ms/step - loss: 0.2581 - accuracy: 0.8976\n",
      "Accuracy: 87.82%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:14:17 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation= \"sigmoid\" ))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[ \"accuracy\" ])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a0f3dee",
   "metadata": {},
   "source": [
    "You can see dropout having the desired impact on training with a slightly slower trend in convergence and, in this case, a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (try it and see)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f46dfe",
   "metadata": {},
   "source": [
    "### LSTM For Sequence Classification With Dropout set with Epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c4cbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 338s 859ms/step - loss: 0.4878 - accuracy: 0.7553\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 764s 2s/step - loss: 0.3097 - accuracy: 0.8766\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 349s 894ms/step - loss: 0.2677 - accuracy: 0.8949\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 361s 925ms/step - loss: 0.2334 - accuracy: 0.9106\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 333s 853ms/step - loss: 0.2211 - accuracy: 0.9161\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 333s 853ms/step - loss: 0.1919 - accuracy: 0.9296\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 547s 1s/step - loss: 0.1886 - accuracy: 0.9292\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 364s 931ms/step - loss: 0.1643 - accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 290s 742ms/step - loss: 0.1468 - accuracy: 0.9453\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 309s 791ms/step - loss: 0.1427 - accuracy: 0.9485\n",
      "Accuracy: 86.63%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 1:07:46 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation= \"sigmoid\" ))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[ \"accuracy\" ])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c34d379",
   "metadata": {},
   "source": [
    "Execution took: 1:07:46 secs (Wall clock time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "It appears model may achieve a higher skill with probably a few more epochs of training, but needs much more than 10 epochs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbced285",
   "metadata": {},
   "source": [
    "Alternately, dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
    "\n",
    "Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout, and recurrent_dropout for configuring the recurrent dropout. For example, you can modify the first example to add dropout to the input and recurrent connections as follows:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "The full code listing with more precise LSTM dropout is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "607d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 327s 828ms/step - loss: 0.4883 - accuracy: 0.7712\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 356s 912ms/step - loss: 0.3353 - accuracy: 0.8625\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 377s 965ms/step - loss: 0.3342 - accuracy: 0.8585\n",
      "Accuracy: 72.21%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:19:14 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea532145",
   "metadata": {},
   "source": [
    "Execution took: 0:19:14 secs (Wall clock time)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1200cc5",
   "metadata": {},
   "source": [
    "The LSTM-specific dropout are known to have a more pronounced effect on the convergence of the network than the layer-wise dropout (see the results on the website link listed in header), but the above results differed. Also, the number of epochs was kept constant in this run of the model and could be increased to see if the skill of the model could be further lifted.\n",
    "\n",
    "Dropout is a powerful technique for combating overfitting in your LSTM models, and it is a good idea to try both methods. Still, you may get better results with the gate-specific dropout provided in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82592003",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e625954",
   "metadata": {},
   "source": [
    "Sometimes, a sequence is better used in reversed order. In those cases, you can simply reverse a vector x using the Python syntax x[::-1] before using it to train your LSTM network.\n",
    "\n",
    "Sometimes, neither the forward nor the reversed order works perfectly, but combining them will give better results. In this case, you will need a bidirectional LSTM network.\n",
    "\n",
    "A bidirectional LSTM network is simply two separate LSTM networks; one feeds with a forward sequence and another with reversed sequence. Then the output of the two LSTM networks is concatenated together before being fed to the subsequent layers of the network. In Keras, you have the function Bidirectional() to clone an LSTM layer for forward-backward input and concatenate their output. For example,\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "Since you created not one, but two LSTMs with 100 units each, this network will take twice the amount of time to train. Depending on the problem, this additional cost may be justified.\n",
    "\n",
    "The full code listing with adding the bidirectional LSTM to the last example is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd02ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              106400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,601\n",
      "Trainable params: 266,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 1622s 4s/step - loss: 0.4820 - accuracy: 0.7723\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 1824s 5s/step - loss: 0.3181 - accuracy: 0.8731\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 1758s 4s/step - loss: 0.3519 - accuracy: 0.8485\n",
      "Accuracy: 79.72%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 1:31:59 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5810a16b",
   "metadata": {},
   "source": [
    "# Comparing against LSTM without dropout run with epochs set to 3 (accuracy = 87.82%), it seems corresponding bidirectional LSTM only yields a slightly poor model skill (79.72%) despite a significantly longer training time. Often the bidirectional LSTM yields a slight improvement in model skill (see the output on website link provided in heading), again but at the cost of a longer training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9e57d",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM for Sequence Classification set with Epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5fce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              106400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,601\n",
      "Trainable params: 266,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "391/391 [==============================] - 1898s 5s/step - loss: 0.5052 - accuracy: 0.7498\n",
      "Epoch 2/25\n",
      "391/391 [==============================] - 1577s 4s/step - loss: 0.3143 - accuracy: 0.8748\n",
      "Epoch 3/25\n",
      "391/391 [==============================] - 2494s 6s/step - loss: 0.2825 - accuracy: 0.8880\n",
      "Epoch 4/25\n",
      "391/391 [==============================] - 2024s 5s/step - loss: 0.2706 - accuracy: 0.8924\n",
      "Epoch 5/25\n",
      "391/391 [==============================] - 9092s 23s/step - loss: 0.2429 - accuracy: 0.9050\n",
      "Epoch 6/25\n",
      "391/391 [==============================] - 1722s 4s/step - loss: 0.1977 - accuracy: 0.9254\n",
      "Epoch 7/25\n",
      "391/391 [==============================] - 1914s 5s/step - loss: 0.1830 - accuracy: 0.9290\n",
      "Epoch 8/25\n",
      "391/391 [==============================] - 2948s 8s/step - loss: 0.1764 - accuracy: 0.9342\n",
      "Epoch 9/25\n",
      "391/391 [==============================] - 2643s 7s/step - loss: 0.1538 - accuracy: 0.9415\n",
      "Epoch 10/25\n",
      "391/391 [==============================] - 1982s 5s/step - loss: 0.1278 - accuracy: 0.9532\n",
      "Epoch 11/25\n",
      "391/391 [==============================] - 2119s 5s/step - loss: 0.1189 - accuracy: 0.9564\n",
      "Epoch 12/25\n",
      "391/391 [==============================] - 2083s 5s/step - loss: 0.1559 - accuracy: 0.9416\n",
      "Epoch 13/25\n",
      "391/391 [==============================] - 4866s 12s/step - loss: 0.1085 - accuracy: 0.9609\n",
      "Epoch 14/25\n",
      "391/391 [==============================] - 2719s 7s/step - loss: 0.1159 - accuracy: 0.9578\n",
      "Epoch 15/25\n",
      "391/391 [==============================] - 2042s 5s/step - loss: 0.0947 - accuracy: 0.9662\n",
      "Epoch 16/25\n",
      "391/391 [==============================] - 36975s 95s/step - loss: 0.0795 - accuracy: 0.9726\n",
      "Epoch 17/25\n",
      "391/391 [==============================] - 2590s 7s/step - loss: 0.0857 - accuracy: 0.9706\n",
      "Epoch 18/25\n",
      "391/391 [==============================] - 2895s 7s/step - loss: 0.0728 - accuracy: 0.9744\n",
      "Epoch 19/25\n",
      "391/391 [==============================] - 6353s 16s/step - loss: 0.0675 - accuracy: 0.9786\n",
      "Epoch 20/25\n",
      "391/391 [==============================] - 2112s 5s/step - loss: 0.0602 - accuracy: 0.9795\n",
      "Epoch 21/25\n",
      "391/391 [==============================] - 2058s 5s/step - loss: 0.0503 - accuracy: 0.9839\n",
      "Epoch 22/25\n",
      "391/391 [==============================] - 4888s 13s/step - loss: 0.0498 - accuracy: 0.9839\n",
      "Epoch 23/25\n",
      "391/391 [==============================] - 1890s 5s/step - loss: 0.0927 - accuracy: 0.9672\n",
      "Epoch 24/25\n",
      "391/391 [==============================] - 1514s 4s/step - loss: 0.0554 - accuracy: 0.9818\n",
      "Epoch 25/25\n",
      "391/391 [==============================] - 1207s 3s/step - loss: 0.0827 - accuracy: 0.9730\n",
      "Accuracy: 85.36%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 1 day, 5:07:23 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=25, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9574264b",
   "metadata": {},
   "source": [
    "Execution took: 1 day, 5:07:23 secs (Wall clock time)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb8a0feb",
   "metadata": {},
   "source": [
    "Relative to the bidirectional LSTM ran for 3 epochs, a corresponding model with 25 epocs improved model performance by 6% (79% to 85%) Thus increasing epochs help gain tremendous improvement but with a significantly longer training time (epochs = 25)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ecf2d",
   "metadata": {},
   "source": [
    "# LSTM and Convolutional Neural Network for Sequence Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0949b55f",
   "metadata": {},
   "source": [
    "Convolutional neural networks excel at learning the spatial structure in input data.\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews, and the CNN may be able to pick out invariant features for the good and bad sentiment. This learned spatial feature may then be learned as sequences by an LSTM layer.\n",
    "\n",
    "You can easily add a one-dimensional CNN and max pooling layers after the Embedding layer, which then feeds the consolidated features to the LSTM. You can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size.\n",
    "\n",
    "For example, you would create the model as follows:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "We will fit the model with 3 epochs, to save time.\n",
    "\n",
    "The full code listing with CNN and LSTM layers is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8c540e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:00:00 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 250, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 276s 699ms/step - loss: 0.4318 - accuracy: 0.7852\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 186s 475ms/step - loss: 0.2473 - accuracy: 0.9033\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 194s 497ms/step - loss: 0.2006 - accuracy: 0.9236\n",
      "Accuracy: 87.67%\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 0:11:03 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfafd943",
   "metadata": {},
   "source": [
    "You can see that you achieve slightly better results than the first example, although with fewer weights and faster training time.\n",
    "\n",
    "You might expect that even better results could be achieved if this example was further extended to use dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7448915",
   "metadata": {},
   "source": [
    "### LSTM and Convolutional Neural Network with dropout for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd88ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:00:00 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 250, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 304s 768ms/step - loss: 0.4577 - accuracy: 0.7718\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 313s 800ms/step - loss: 0.2525 - accuracy: 0.9007\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 241s 616ms/step - loss: 0.2106 - accuracy: 0.9198\n",
      "Accuracy: 88.04%\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 0:15:24 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# LSTM and CNN with dropout for sequence classification in the IMDB dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b864209d",
   "metadata": {},
   "source": [
    "LSTM CNN with dropout improved the performace only very slightly. Running for more epochs may improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac28907",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "baef11a2",
   "metadata": {},
   "source": [
    "In this post, you discovered how to develop LSTM network models for sequence classification predictive modeling problems.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "How to develop a simple single-layer LSTM model for the IMDB movie review sentiment classification problem\n",
    "How to extend your LSTM model with layer-wise and LSTM-specific dropout to reduce overfitting\n",
    "How to combine the spatial structure learning properties of a Convolutional Neural Network with the sequence learning of an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b89d4",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476bdea4",
   "metadata": {},
   "source": [
    "In completing this project you learned how you can use LSTM recurrent neural networks for sequence classification. In the next lesson you will build upon your understanding of\n",
    "LSTM networks and better understand how the Keras API maintains state for simple sequence prediction problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
