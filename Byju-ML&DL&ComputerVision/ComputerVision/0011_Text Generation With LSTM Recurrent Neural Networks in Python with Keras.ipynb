{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cd27a8",
   "metadata": {},
   "source": [
    "# Text Generation With LSTM Recurrent Neural Networks in Python with Keras\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f021f95",
   "metadata": {},
   "source": [
    "Recurrent neural networks can also be used as generative models.\n",
    "\n",
    "This means that in addition to being used for predictive models (making predictions), they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain.\n",
    "\n",
    "Generative models like this are useful not only to study how well a model has learned a problem but also to learn more about the problem domain itself.\n",
    "\n",
    "In this post, you will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Python with Keras.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "Where to download a free corpus of text that you can use to train text generative models\n",
    "How to frame the problem of text sequences to a recurrent neural network generative model\n",
    "How to develop an LSTM to generate plausible text sequences for a given problem\n",
    "Kick-start your project with my new book Deep Learning for Natural Language Processing, including step-by-step tutorials and the Python source code files for all examples.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "Note: LSTM recurrent neural networks can be slow to train, and it is highly recommended that you train them on GPU hardware. You can access GPU hardware in the cloud very cheaply using Amazon Web Services. See the tutorial here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665c36f",
   "metadata": {},
   "source": [
    "# Problem Description: Project Gutenberg"
   ]
  },
  {
   "cell_type": "raw",
   "id": "580dabef",
   "metadata": {},
   "source": [
    "Many of the classical texts are no longer protected under copyright.\n",
    "\n",
    "This means you can download all the text for these books for free and use them in experiments, like creating generative models. Perhaps the best place to get access to free books that are no longer protected by copyright is Project Gutenberg.\n",
    "\n",
    "In this tutorial, you will use a favorite book from childhood as the dataset: Alice’s Adventures in Wonderland by Lewis Carroll.\n",
    "\n",
    "You will learn the dependencies between characters and the conditional probabilities of characters in sequences so that you can, in turn, generate wholly new and original sequences of characters.\n",
    "\n",
    "This is a lot of fun, and repeating these experiments with other books from Project Gutenberg is recommended. Here is a list of the most popular books on the site.\n",
    "\n",
    "These experiments are not limited to text; you can also experiment with other ASCII data, such as computer source code, marked-up documents in LaTeX, HTML or Markdown, and more.\n",
    "\n",
    "You can download the complete text in ASCII format (Plain Text UTF-8) for this book for free and place it in your working directory with the filename wonderland.txt.\n",
    "\n",
    "Now, you need to prepare the dataset ready for modeling.\n",
    "\n",
    "Project Gutenberg adds a standard header and footer to each book, which is not part of the original text. Open the file in a text editor and delete the header and footer.\n",
    "\n",
    "The header is obvious and ends with the text:\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
    "The footer is all the text after the line of text that says:\n",
    "\n",
    "THE END\n",
    "You should be left with a text file that has about 3,330 lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c74f9",
   "metadata": {},
   "source": [
    "# Develop a Small LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed56192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In this section, you will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section, you will use this model to generate new sequences of characters.\n",
    "\n",
    "Let’s start by importing the classes and functions you will use to train your model.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "...\n",
    "Next, you need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary the network must learn.\n",
    "\n",
    "...\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "Now that the book is loaded, you must prepare the data for modeling by the neural network. You cannot model the characters directly; instead, you must convert the characters to integers.\n",
    "\n",
    "You can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer.\n",
    "\n",
    "...\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "For example, the list of unique sorted lowercase characters in the book is as follows:\n",
    "\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']\n",
    "You can see that there may be some characters that we could remove to further clean up the dataset to reduce the vocabulary, which may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, you can summarize the dataset.\n",
    "\n",
    "...\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "Running the code to this point produces the following output.\n",
    "\n",
    "Total Characters:  147674\n",
    "Total Vocab:  47\n",
    "You can see the book has just under 150,000 characters, and when converted to lowercase, there are only 47 distinct characters in the vocabulary for the network to learn—much more than the 26 in the alphabet.\n",
    "\n",
    "You now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial, you will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. You could just as easily split the data by sentences, padding the shorter sequences and truncating the longer ones.\n",
    "\n",
    "Each training pattern of the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, you slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters, of course).\n",
    "\n",
    "For example, if the sequence length is 5 (for simplicity), then the first two training patterns would be as follows:\n",
    "\n",
    "CHAPT -> E\n",
    "HAPTE -> R\n",
    "As you split the book into these sequences, you convert the characters to integers using the lookup table you prepared earlier.\n",
    "\n",
    "...\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "Running the code to this point shows that when you split up the dataset into training data for the network to learn that you have just under 150,000 training patterns. This makes sense as, excluding the first 100 characters, you have one training pattern to predict each of the remaining characters.\n",
    "\n",
    "Total Patterns:  147574\n",
    "Now that you have prepared your training data, you need to transform it to be suitable for use with Keras.\n",
    "\n",
    "First, you must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next, you need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network using the sigmoid activation function by default.\n",
    "\n",
    "Finally, you need to convert the output patterns (single characters converted to integers) into a one-hot encoding. This is so that you can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros, except with a 1 in the column for the letter (integer) that the pattern represents.\n",
    "\n",
    "For example, when “n” (integer value 31) is one-hot encoded, it looks as follows:\n",
    "\n",
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
    "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "You can implement these steps as below:\n",
    "\n",
    "...\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "You can now define your LSTM model. Here, you define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with 47 classes and, as such, is defined as optimizing the log loss (cross entropy) using the ADAM optimization algorithm for speed.\n",
    "\n",
    "...\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "There is no test dataset. You are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "You are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead, you are interested in a generalization of the dataset that minimizes the chosen loss function. You are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of the optimization requirements, use model checkpointing to record all the network weights to file each time an improvement in loss is observed at the end of the epoch. You will use the best set of weights (lowest loss) to instantiate your generative model in the next section.\n",
    "\n",
    "...\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "You can now fit your model to the data. Here, you use a modest number of 20 epochs and a large batch size of 128 patterns.\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be48356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144449\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144349\n",
      "Epoch 1/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.9862\n",
      "Epoch 1: loss improved from inf to 2.98624, saving model to weights-improvement-01-2.9862.hdf5\n",
      "1128/1128 [==============================] - 486s 428ms/step - loss: 2.9862\n",
      "Epoch 2/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.8504\n",
      "Epoch 2: loss improved from 2.98624 to 2.85036, saving model to weights-improvement-02-2.8504.hdf5\n",
      "1128/1128 [==============================] - 515s 457ms/step - loss: 2.8504\n",
      "Epoch 3/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7903\n",
      "Epoch 3: loss improved from 2.85036 to 2.79026, saving model to weights-improvement-03-2.7903.hdf5\n",
      "1128/1128 [==============================] - 391s 347ms/step - loss: 2.7903\n",
      "Epoch 4/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7284\n",
      "Epoch 4: loss improved from 2.79026 to 2.72844, saving model to weights-improvement-04-2.7284.hdf5\n",
      "1128/1128 [==============================] - 385s 341ms/step - loss: 2.7284\n",
      "Epoch 5/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6827\n",
      "Epoch 5: loss improved from 2.72844 to 2.68266, saving model to weights-improvement-05-2.6827.hdf5\n",
      "1128/1128 [==============================] - 382s 339ms/step - loss: 2.6827\n",
      "Epoch 6/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6288\n",
      "Epoch 6: loss improved from 2.68266 to 2.62878, saving model to weights-improvement-06-2.6288.hdf5\n",
      "1128/1128 [==============================] - 366s 325ms/step - loss: 2.6288\n",
      "Epoch 7/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5734\n",
      "Epoch 7: loss improved from 2.62878 to 2.57342, saving model to weights-improvement-07-2.5734.hdf5\n",
      "1128/1128 [==============================] - 369s 327ms/step - loss: 2.5734\n",
      "Epoch 8/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5220\n",
      "Epoch 8: loss improved from 2.57342 to 2.52199, saving model to weights-improvement-08-2.5220.hdf5\n",
      "1128/1128 [==============================] - 369s 327ms/step - loss: 2.5220\n",
      "Epoch 9/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4744\n",
      "Epoch 9: loss improved from 2.52199 to 2.47440, saving model to weights-improvement-09-2.4744.hdf5\n",
      "1128/1128 [==============================] - 370s 328ms/step - loss: 2.4744\n",
      "Epoch 10/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4287\n",
      "Epoch 10: loss improved from 2.47440 to 2.42871, saving model to weights-improvement-10-2.4287.hdf5\n",
      "1128/1128 [==============================] - 372s 330ms/step - loss: 2.4287\n",
      "Epoch 11/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3853\n",
      "Epoch 11: loss improved from 2.42871 to 2.38525, saving model to weights-improvement-11-2.3853.hdf5\n",
      "1128/1128 [==============================] - 374s 331ms/step - loss: 2.3853\n",
      "Epoch 12/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3447\n",
      "Epoch 12: loss improved from 2.38525 to 2.34472, saving model to weights-improvement-12-2.3447.hdf5\n",
      "1128/1128 [==============================] - 372s 329ms/step - loss: 2.3447\n",
      "Epoch 13/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3061\n",
      "Epoch 13: loss improved from 2.34472 to 2.30605, saving model to weights-improvement-13-2.3061.hdf5\n",
      "1128/1128 [==============================] - 383s 339ms/step - loss: 2.3061\n",
      "Epoch 14/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2675\n",
      "Epoch 14: loss improved from 2.30605 to 2.26750, saving model to weights-improvement-14-2.2675.hdf5\n",
      "1128/1128 [==============================] - 374s 331ms/step - loss: 2.2675\n",
      "Epoch 15/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2309\n",
      "Epoch 15: loss improved from 2.26750 to 2.23093, saving model to weights-improvement-15-2.2309.hdf5\n",
      "1128/1128 [==============================] - 373s 330ms/step - loss: 2.2309\n",
      "Epoch 16/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1960\n",
      "Epoch 16: loss improved from 2.23093 to 2.19602, saving model to weights-improvement-16-2.1960.hdf5\n",
      "1128/1128 [==============================] - 317s 281ms/step - loss: 2.1960\n",
      "Epoch 17/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1598\n",
      "Epoch 17: loss improved from 2.19602 to 2.15977, saving model to weights-improvement-17-2.1598.hdf5\n",
      "1128/1128 [==============================] - 322s 285ms/step - loss: 2.1598\n",
      "Epoch 18/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1269\n",
      "Epoch 18: loss improved from 2.15977 to 2.12693, saving model to weights-improvement-18-2.1269.hdf5\n",
      "1128/1128 [==============================] - 328s 291ms/step - loss: 2.1269\n",
      "Epoch 19/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0978\n",
      "Epoch 19: loss improved from 2.12693 to 2.09781, saving model to weights-improvement-19-2.0978.hdf5\n",
      "1128/1128 [==============================] - 327s 290ms/step - loss: 2.0978\n",
      "Epoch 20/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0666\n",
      "Epoch 20: loss improved from 2.09781 to 2.06660, saving model to weights-improvement-20-2.0666.hdf5\n",
      "1128/1128 [==============================] - 337s 299ms/step - loss: 2.0666\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 2:05:16 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "510c9467",
   "metadata": {},
   "source": [
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value. For example, when this example was run, you can see below the checkpoint with the smallest loss that was achieved.\n",
    "\n",
    "weights-improvement-20-2.0666.hdf5\n",
    "The network loss decreased almost every epoch, so the network could likely benefit from training for many more epochs.\n",
    "\n",
    "In the next section, you will look at using this model to generate new text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0d967",
   "metadata": {},
   "source": [
    "# Generating Text with an LSTM Network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "657f0abe",
   "metadata": {},
   "source": [
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "First, you will load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file, and the network does not need to be trained.\n",
    "\n",
    "...\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.0666.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "Also, when preparing the mapping of unique characters to integers, you must also create a reverse mapping that you can use to convert the integers back to characters so that you can understand the predictions.\n",
    "\n",
    "...\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "Finally, you need to actually make predictions.\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start with a seed sequence as input, generate the next character, then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as you want to predict new characters (e.g., a sequence of 1,000 characters in length).\n",
    "\n",
    "You can pick a random input pattern as your seed sequence, then print generated characters as you generate them.\n",
    "\n",
    "...\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "The full code example for generating text using the loaded LSTM model is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5a464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144449\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144349\n",
      "Seed:\n",
      "\" n,' the king said to the jury, and the jury eagerly\n",
      "wrote down all three dates on their slates, and  \"\n",
      "the sas aoinged at the huuphon and the was so tee whet she was soe that she was soe tiat she was so the wool at the could,\n",
      "\n",
      "'the had toin he so tel ' she katter weit on, ''i mever tan a taryer ' said the matth rare tery alliilyyyy. 'iu saan the mort of the sooe-'\n",
      "\n",
      "'i dane tat a gind ' said the match hare.\n",
      "\n",
      "'it so tee ' she katter weit on, ''  '                                                                                             *  *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *  \n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 0:01:19 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.0666.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8580d2e",
   "metadata": {},
   "source": [
    "Running this example first outputs the selected random seed, then each character as it is generated.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "For example, below are the results from one run of this text generator. The random seed was:\n",
    "\n",
    "be no mistake about it: it was neither more nor less than a pig, and she\n",
    "felt that it would be quit\n",
    "The generated text with the random seed (cleaned up for presentation) was:\n",
    "\n",
    "be no mistake about it: it was neither more nor less than a pig, and she\n",
    "felt that it would be quit e aelin that she was a little want oe toiet\n",
    "ano a grtpersent to the tas a little war th tee the tase oa teettee\n",
    "the had been tinhgtt a little toiee at the cadl in a long tuiee aedun\n",
    "thet sheer was a little tare gereen to be a gentle of the tabdit  soenee\n",
    "the gad  ouw ie the tay a tirt of toiet at the was a little \n",
    "anonersen, and thiu had been woite io a lott of tueh a tiie  and taede\n",
    "bot her aeain  she cere thth the bene tith the tere bane to tee\n",
    "toaete to tee the harter was a little tire the same oare cade an anl ano\n",
    "the garee and the was so seat the was a little gareen and the sabdit,\n",
    "and the white rabbit wese tilel an the caoe and the sabbit se teeteer,\n",
    "and the white rabbit wese tilel an the cade in a lonk tfne the sabdi\n",
    "ano aroing to tea the was sf teet whitg the was a little tane oo thete\n",
    "the sabeit  she was a little tartig to the tar tf tee the tame of the\n",
    "cagd, and the white rabbit was a little toiee to be anle tite thete ofs\n",
    "and the tabdit was the wiite rabbit, and\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4959bd32",
   "metadata": {},
   "source": [
    "Let’s note some observations about the generated text.\n",
    "\n",
    "It generally conforms to the line format observed in the original text of fewer than 80 characters before a new line.\n",
    "The characters are separated into word-like groups, and most groups are actual English words (e.g., “the,” “little,” and “was”), but many are not (e.g., “lott,” “tiie,” and “taede”).\n",
    "Some of the words in sequence make sense(e.g., “and the white rabbit“), but many do not (e.g., “wese tilel“).\n",
    "The fact that this character-based model of the book produces output like this is very impressive. It gives you a sense of the learning capabilities of LSTM networks.\n",
    "\n",
    "However, the results are not perfect.\n",
    "\n",
    "In the next section, you will look at improving the quality of results by developing a much larger LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b3dbb",
   "metadata": {},
   "source": [
    "# Larger LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2773344",
   "metadata": {},
   "source": [
    "\n",
    "You got results, but not excellent results in the previous section. Now, you can try to improve the quality of the generated text by creating a much larger network.\n",
    "\n",
    "You will keep the number of memory units the same at 256 but add a second layer.\n",
    "\n",
    "...\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "You will also change the filename of the checkpointed weights so that you can tell the difference between weights for this network and the previous (by appending the word “bigger” in the filename).\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "Finally, you will increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn.\n",
    "\n",
    "The full code listing is presented below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5b39da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144449\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144349\n",
      "Epoch 1/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.7617\n",
      "Epoch 1: loss improved from inf to 2.76175, saving model to weights-improvement-01-2.7617-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1442s 638ms/step - loss: 2.7617\n",
      "Epoch 2/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.3972\n",
      "Epoch 2: loss improved from 2.76175 to 2.39716, saving model to weights-improvement-02-2.3972-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 17459s 8s/step - loss: 2.3972\n",
      "Epoch 3/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.1993\n",
      "Epoch 3: loss improved from 2.39716 to 2.19928, saving model to weights-improvement-03-2.1993-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1349s 598ms/step - loss: 2.1993\n",
      "Epoch 4/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.0769\n",
      "Epoch 4: loss improved from 2.19928 to 2.07692, saving model to weights-improvement-04-2.0769-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1110s 492ms/step - loss: 2.0769\n",
      "Epoch 5/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.9802\n",
      "Epoch 5: loss improved from 2.07692 to 1.98022, saving model to weights-improvement-05-1.9802-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1062s 471ms/step - loss: 1.9802\n",
      "Epoch 6/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.9046\n",
      "Epoch 6: loss improved from 1.98022 to 1.90455, saving model to weights-improvement-06-1.9046-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1087s 482ms/step - loss: 1.9046\n",
      "Epoch 7/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.8350\n",
      "Epoch 7: loss improved from 1.90455 to 1.83504, saving model to weights-improvement-07-1.8350-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1160s 514ms/step - loss: 1.8350\n",
      "Epoch 8/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.7832\n",
      "Epoch 8: loss improved from 1.83504 to 1.78321, saving model to weights-improvement-08-1.7832-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1196s 530ms/step - loss: 1.7832\n",
      "Epoch 9/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.7365\n",
      "Epoch 9: loss improved from 1.78321 to 1.73655, saving model to weights-improvement-09-1.7365-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1213s 538ms/step - loss: 1.7365\n",
      "Epoch 10/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.6930\n",
      "Epoch 10: loss improved from 1.73655 to 1.69296, saving model to weights-improvement-10-1.6930-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 995s 441ms/step - loss: 1.6930\n",
      "Epoch 11/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.6577\n",
      "Epoch 11: loss improved from 1.69296 to 1.65773, saving model to weights-improvement-11-1.6577-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 960s 425ms/step - loss: 1.6577\n",
      "Epoch 12/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.0073\n",
      "Epoch 12: loss did not improve from 1.65773\n",
      "2256/2256 [==============================] - 952s 422ms/step - loss: 2.0073\n",
      "Epoch 13/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.5966\n",
      "Epoch 13: loss did not improve from 1.65773\n",
      "2256/2256 [==============================] - 948s 420ms/step - loss: 2.5966\n",
      "Epoch 14/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 2.1637\n",
      "Epoch 14: loss did not improve from 1.65773\n",
      "2256/2256 [==============================] - 1103s 489ms/step - loss: 2.1637\n",
      "Epoch 15/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.7781\n",
      "Epoch 15: loss did not improve from 1.65773\n",
      "2256/2256 [==============================] - 1233s 546ms/step - loss: 1.7781\n",
      "Epoch 16/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.6500\n",
      "Epoch 16: loss improved from 1.65773 to 1.64996, saving model to weights-improvement-16-1.6500-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1317s 583ms/step - loss: 1.6500\n",
      "Epoch 17/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.5974\n",
      "Epoch 17: loss improved from 1.64996 to 1.59739, saving model to weights-improvement-17-1.5974-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1368s 606ms/step - loss: 1.5974\n",
      "Epoch 18/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.5599\n",
      "Epoch 18: loss improved from 1.59739 to 1.55994, saving model to weights-improvement-18-1.5599-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1362s 603ms/step - loss: 1.5599\n",
      "Epoch 19/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.5282\n",
      "Epoch 19: loss improved from 1.55994 to 1.52824, saving model to weights-improvement-19-1.5282-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 2258s 1s/step - loss: 1.5282\n",
      "Epoch 20/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.5052\n",
      "Epoch 20: loss improved from 1.52824 to 1.50521, saving model to weights-improvement-20-1.5052-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1688s 748ms/step - loss: 1.5052\n",
      "Epoch 21/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.4811\n",
      "Epoch 21: loss improved from 1.50521 to 1.48105, saving model to weights-improvement-21-1.4811-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1420s 629ms/step - loss: 1.4811\n",
      "Epoch 22/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 22: loss improved from 1.48105 to 1.45877, saving model to weights-improvement-22-1.4588-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1518s 673ms/step - loss: 1.4588\n",
      "Epoch 23/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 23: loss improved from 1.45877 to 1.44079, saving model to weights-improvement-23-1.4408-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 20008s 9s/step - loss: 1.4408\n",
      "Epoch 24/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 24: loss improved from 1.44079 to 1.42198, saving model to weights-improvement-24-1.4220-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1312s 581ms/step - loss: 1.4220\n",
      "Epoch 25/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.4077\n",
      "Epoch 25: loss improved from 1.42198 to 1.40773, saving model to weights-improvement-25-1.4077-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1258s 558ms/step - loss: 1.4077\n",
      "Epoch 26/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3923\n",
      "Epoch 26: loss improved from 1.40773 to 1.39232, saving model to weights-improvement-26-1.3923-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1259s 558ms/step - loss: 1.3923\n",
      "Epoch 27/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3750\n",
      "Epoch 27: loss improved from 1.39232 to 1.37502, saving model to weights-improvement-27-1.3750-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1262s 559ms/step - loss: 1.3750\n",
      "Epoch 28/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3622\n",
      "Epoch 28: loss improved from 1.37502 to 1.36216, saving model to weights-improvement-28-1.3622-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1103s 489ms/step - loss: 1.3622\n",
      "Epoch 29/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3475\n",
      "Epoch 29: loss improved from 1.36216 to 1.34747, saving model to weights-improvement-29-1.3475-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1108s 491ms/step - loss: 1.3475\n",
      "Epoch 30/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3352\n",
      "Epoch 30: loss improved from 1.34747 to 1.33521, saving model to weights-improvement-30-1.3352-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1133s 502ms/step - loss: 1.3352\n",
      "Epoch 31/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3252\n",
      "Epoch 31: loss improved from 1.33521 to 1.32524, saving model to weights-improvement-31-1.3252-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1294s 574ms/step - loss: 1.3252\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3165\n",
      "Epoch 32: loss improved from 1.32524 to 1.31654, saving model to weights-improvement-32-1.3165-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1201s 532ms/step - loss: 1.3165\n",
      "Epoch 33/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.3055\n",
      "Epoch 33: loss improved from 1.31654 to 1.30553, saving model to weights-improvement-33-1.3055-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1144s 507ms/step - loss: 1.3055\n",
      "Epoch 34/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2948\n",
      "Epoch 34: loss improved from 1.30553 to 1.29477, saving model to weights-improvement-34-1.2948-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1160s 514ms/step - loss: 1.2948\n",
      "Epoch 35/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2871\n",
      "Epoch 35: loss improved from 1.29477 to 1.28707, saving model to weights-improvement-35-1.2871-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1180s 523ms/step - loss: 1.2871\n",
      "Epoch 36/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2800\n",
      "Epoch 36: loss improved from 1.28707 to 1.27998, saving model to weights-improvement-36-1.2800-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1249s 554ms/step - loss: 1.2800\n",
      "Epoch 37/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2671\n",
      "Epoch 37: loss improved from 1.27998 to 1.26706, saving model to weights-improvement-37-1.2671-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 4786s 2s/step - loss: 1.2671\n",
      "Epoch 38/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2631\n",
      "Epoch 38: loss improved from 1.26706 to 1.26310, saving model to weights-improvement-38-1.2631-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1207s 535ms/step - loss: 1.2631\n",
      "Epoch 39/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2565\n",
      "Epoch 39: loss improved from 1.26310 to 1.25653, saving model to weights-improvement-39-1.2565-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1218s 540ms/step - loss: 1.2565\n",
      "Epoch 40/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2497\n",
      "Epoch 40: loss improved from 1.25653 to 1.24966, saving model to weights-improvement-40-1.2497-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 4826s 2s/step - loss: 1.2497\n",
      "Epoch 41/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2430\n",
      "Epoch 41: loss improved from 1.24966 to 1.24299, saving model to weights-improvement-41-1.2430-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1217s 539ms/step - loss: 1.2430\n",
      "Epoch 42/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2399\n",
      "Epoch 42: loss improved from 1.24299 to 1.23988, saving model to weights-improvement-42-1.2399-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1225s 543ms/step - loss: 1.2399\n",
      "Epoch 43/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2332\n",
      "Epoch 43: loss improved from 1.23988 to 1.23318, saving model to weights-improvement-43-1.2332-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 4195s 2s/step - loss: 1.2332\n",
      "Epoch 44/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2329\n",
      "Epoch 44: loss improved from 1.23318 to 1.23289, saving model to weights-improvement-44-1.2329-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1582s 701ms/step - loss: 1.2329\n",
      "Epoch 45/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2248\n",
      "Epoch 45: loss improved from 1.23289 to 1.22485, saving model to weights-improvement-45-1.2248-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1289s 571ms/step - loss: 1.2248\n",
      "Epoch 46/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2210\n",
      "Epoch 46: loss improved from 1.22485 to 1.22098, saving model to weights-improvement-46-1.2210-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1244s 551ms/step - loss: 1.2210\n",
      "Epoch 47/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2108\n",
      "Epoch 47: loss improved from 1.22098 to 1.21076, saving model to weights-improvement-47-1.2108-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1419s 629ms/step - loss: 1.2108\n",
      "Epoch 48/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2122\n",
      "Epoch 48: loss did not improve from 1.21076\n",
      "2256/2256 [==============================] - 1511s 670ms/step - loss: 1.2122\n",
      "Epoch 49/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2041\n",
      "Epoch 49: loss improved from 1.21076 to 1.20405, saving model to weights-improvement-49-1.2041-bigger-model.hdf5\n",
      "2256/2256 [==============================] - 1845s 818ms/step - loss: 1.2041\n",
      "Epoch 50/50\n",
      "2256/2256 [==============================] - ETA: 0s - loss: 1.2143\n",
      "Epoch 50: loss did not improve from 1.20405\n",
      "2256/2256 [==============================] - 1558s 690ms/step - loss: 1.2143\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 1 day, 6:16:37 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger-model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6773f910",
   "metadata": {},
   "source": [
    "After running this example, you may achieve a loss of about 1.2. For example, the best result achieved from running this model was stored in a checkpoint file with the name:\n",
    "\n",
    "weights-improvement-49-1.2041-bigger-model.hdf5\n",
    "This achieved a loss of 1.2041 at epoch 49.\n",
    "\n",
    "As in the previous section, you can use this best model from the run to generate text.\n",
    "\n",
    "The only change you need to make to the text generation script from the previous section is in the specification of the network topology and from which file to seed the network weights.\n",
    "\n",
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e7c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144449\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144349\n",
      "Seed:\n",
      "\"  this\n",
      "moment, i tell you!' but she went on all the same, shedding gallons of\n",
      "tears, until there was  \"\n",
      "no rseer the words as the cook and looked anxiously another get on their faces, and the three gardeners in the doumouse said to the gatter. \n",
      "'i don't know the way out of the baniers!' she said to herself, 'i wonder what they were that the mooent the reason it, you know.'\n",
      "\n",
      "'i don't know what to det you wouldn't talk,' said the king. 'and the moral of that is--\"but i must be gate any meter in the bankee of the bankee farher in the door, and the three gardeners in the dodo solenlly langer in the door, she was not and said to the book and looked anxiously and looked anxiously about a lowse in the door, and was going to got some minutes. \n",
      "the dormouse seplied them as the cook as she went on, 'what is the white rereamed about it, and wet i con't know that wou wouldn't talk to be a book and the words again! i'll gave you think you wouldn't be an on the bnowers--and they're all rhe saie that is my beeo tie that is was any onring. \n",
      "\n",
      "'i won't ' said the mock turtle. \n",
      "'i don't know the way out of\n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Execution took: 0:00:53 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-49-1.2041-bigger-model.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd7c2cf5",
   "metadata": {},
   "source": [
    "One other example of running this text generation script produces the output below.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "The randomly chosen seed text was:\n",
    "\n",
    "d herself lying on the bank, with her\n",
    "head in the lap of her sister, who was gently brushing away s\n",
    "The generated text with the seed (cleaned up for presentation) was :\n",
    "\n",
    "herself lying on the bank, with her\n",
    "head in the lap of her sister, who was gently brushing away\n",
    "so siee, and she sabbit said to herself and the sabbit said to herself and the sood\n",
    "way of the was a little that she was a little lad good to the garden,\n",
    "and the sood of the mock turtle said to herself, 'it was a little that\n",
    "the mock turtle said to see it said to sea it said to sea it say it\n",
    "the marge hard sat hn a little that she was so sereated to herself, and\n",
    "she sabbit said to herself, 'it was a little little shated of the sooe\n",
    "of the coomouse it was a little lad good to the little gooder head. and\n",
    "said to herself, 'it was a little little shated of the mouse of the\n",
    "good of the courte, and it was a little little shated in a little that\n",
    "the was a little little shated of the thmee said to see it was a little\n",
    "book of the was a little that she was so sereated to hare a little the\n",
    "began sitee of the was of the was a little that she was so seally and\n",
    "the sabbit was a little lad good to the little gooder head of the gad\n",
    "seared to see it was a little lad good to the little good\n",
    "You can see that there are generally fewer spelling mistakes, and the text looks more realistic but is still quite nonsensical.\n",
    "\n",
    "For example, the same phrases get repeated again and again, like “said to herself” and “little.” Quotes are opened but not closed.\n",
    "\n",
    "These are better results, but there is still a lot of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3e58",
   "metadata": {},
   "source": [
    "# Extension Ideas to Improve the Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb007593",
   "metadata": {},
   "source": [
    "Below are ten ideas that may further improve the model that you could experiment with are:\n",
    "\n",
    "Predict fewer than 1,000 characters as output for a given seed \n",
    "Remove all punctuation from the source text and, therefore, from the models’ vocabulary\n",
    "Try a one-hot encoding for the input sequences\n",
    "Train the model on padded sentences rather than random sequences of characters\n",
    "Increase the number of training epochs to 100 or many hundreds\n",
    "Add dropout to the visible input layer and consider tuning the dropout percentage\n",
    "Tune the batch size; try a batch size of 1 as a (very slow) baseline and larger sizes from there\n",
    "Add more memory units to the layers and/or more layers\n",
    "Experiment with scale factors (temperature) when interpreting the prediction probabilities\n",
    "Change the LSTM layers to be “stateful” to maintain state across batches\n",
    "Did you try any of these extensions? Share your results in the comments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
