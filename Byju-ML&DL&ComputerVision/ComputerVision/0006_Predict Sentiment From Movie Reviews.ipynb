{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f62562",
   "metadata": {},
   "source": [
    "# Predict Sentiment From Movie Reviews\n",
    "Deep Learning with Python: Jason Brownlee, Page 169-175\n",
    "https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83165565",
   "metadata": {},
   "source": [
    "Sentiment analysis is a natural language processing problem where text is understood and the underlying intent is predicted. In this lesson you will discover how you can predict the sentiment of movie reviews as either positive or negative in Python using the Keras deep learning library.\n",
    "\n",
    "After completing this step-by-step tutorial, you will know:\n",
    "\n",
    "About the IMDB sentiment analysis problem for natural language processing and how to load it in Keras.\n",
    "How to use word embedding in Keras for natural language problems.\n",
    "How to develop and evaluate a Multilayer Perceptron model for the IMDB problem.\n",
    "How to develop a one-dimensional convolutional neural network model for the IMDB problem.\n",
    "\n",
    "Let’s get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "Movie Review Sentiment Classification Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cea0388",
   "metadata": {},
   "source": [
    "The dataset used in this project is the Large Movie Review Dataset often referred to as the IMDB dataset1. The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment.\n",
    "\n",
    "The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test2. An accuracy of 88.89% was achieved. The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” from late 2014 to early 2015. Accuracy was achieved above 97%, with winners achieving 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11327279",
   "metadata": {},
   "source": [
    "# Load the IMDB Dataset With Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ae2df83",
   "metadata": {},
   "source": [
    "Keras provides access to the IMDB dataset built-in3. The imdb.load data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of\n",
    "integers.\n",
    "\n",
    "Calling imdb.load data() the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file. Usefully, the imdb.load data() function provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid the repeated use of “the”) and the maximum length of reviews to support. Let’s load the dataset and calculate some properties of it. We will start off by loading some libraries and loading the entire IMDB dataset as a training dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "86830aa5",
   "metadata": {},
   "source": [
    "import numpy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "\n",
    "Next we can display the shape of the training dataset.\n",
    "\n",
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "Running this snippet, we can see that there are 50,000 records.\n",
    "Training data:\n",
    "(50000,)\n",
    "(50000,)\n",
    "\n",
    "We can also print the unique class values.\n",
    "\n",
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))\n",
    "\n",
    "We can see that it is a binary classification problem for good and bad sentiment in the review.\n",
    "Classes:\n",
    "[0 1]\n",
    "\n",
    "Next we can get an idea of the total number of unique words in the dataset.\n",
    "\n",
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "\n",
    "\n",
    "Interestingly, we can see that there are just under 100,000 words across the entire dataset.\n",
    "Number of words:\n",
    "88585\n",
    "\n",
    "\n",
    "Finally, we can get an idea of the average review length.\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "\n",
    "# plot review length as a boxplot and histogram\n",
    "pyplot.subplot(121)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.subplot(122)\n",
    "pyplot.hist(result)\n",
    "pyplot.show()\n",
    "\n",
    "We can see that the average review has just under 300 words with a standard deviation of just over 200 words.\n",
    "Review length:\n",
    "Mean 234.76 words (172.911495)\n",
    "\n",
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1160acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n",
      "Classes: \n",
      "[0 1]\n",
      "Number of words: \n",
      "88585\n",
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and Plot the IMDB dataset\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))\n",
    "\n",
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "\n",
    "\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "\n",
    "\n",
    "# plot review length as a boxplot and histogram\n",
    "pyplot.subplot(121)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.subplot(122)\n",
    "pyplot.hist(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93677c3f",
   "metadata": {},
   "source": [
    "Looking at the box and whisker plot and the histogram for the review lengths in words, we can probably see an exponential distribution that we can probably cover the mass of the distribution with a clipped length of 400 to 500 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2793c3",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70d36379",
   "metadata": {},
   "source": [
    "A recent breakthrough in the field of natural language processing is called word embedding.\n",
    "\n",
    "This technique is where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models as they require numbers as input.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "The layer takes arguments that define the mapping, including the maximum number of expected words, also called the vocabulary size (e.g., the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
    "\n",
    "You want to use a word embedding representation for the IMDB dataset.\n",
    "\n",
    "Let’s say that you are only interested in the first 5,000 most used words in the dataset. Therefore, your vocabulary size will be 5,000. You can choose to use a 32-dimension vector to represent each word. Finally, you may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values.\n",
    "\n",
    "You will load the IMDB dataset as follows:\n",
    "\n",
    "...\n",
    "imdb.load_data(nb_words=5000)\n",
    "You will then use the Keras utility to truncate or pad the dataset to a length of 500 for each observation using the sequence.pad_sequences() function.\n",
    "\n",
    "...\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "Finally, later on, the first layer of your model would be a word embedding layer created using the Embedding class as follows:\n",
    "\n",
    "...\n",
    "Embedding(5000, 32, input_length=500)\n",
    "The output of this first layer would be a matrix with the size 32×500 for a given review training or test pattern in integer format.\n",
    "\n",
    "Now that you know how to load the IMDB dataset in Keras and how to use a word embedding representation for it, let’s develop and evaluate some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30fa537",
   "metadata": {},
   "source": [
    "# Simple Multi-Layer Perceptron Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60b90aaa",
   "metadata": {},
   "source": [
    "\n",
    "You can start by developing a simple multi-layer perceptron model with a single hidden layer.\n",
    "\n",
    "The word embedding representation is a true innovation, and you will demonstrate what would have been considered world-class results in 2011 with a relatively simple neural network.\n",
    "\n",
    "Let’s start by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure you can easily reproduce the results.\n",
    "\n",
    "# MLP for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    " \n",
    "...\n",
    "Next, you will load the IMDB dataset. You will simplify the dataset as discussed during the section on word embeddings—only the top 5,000 words will be loaded.\n",
    "\n",
    "You will also use a 50/50 split of the dataset into training and test sets. This is a good standard split methodology.\n",
    "\n",
    "...\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "You will bound reviews at 500 words, truncating longer reviews and zero-padding shorter ones.\n",
    "\n",
    "...\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "Now, you can create your model. You will use an Embedding layer as the input layer, setting the vocabulary to 5,000, the word vector size to 32 dimensions, and the input_length to 500. The output of this first layer will be a 32×500-sized matrix, as discussed in the previous section.\n",
    "\n",
    "You will flatten the Embedded layers’ output to one dimension, then use one dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions.\n",
    "\n",
    "The model uses logarithmic loss and is optimized using the efficient ADAM optimization procedure.\n",
    "\n",
    "...\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "You can fit the model and use the test set as validation while training. This model overfits very quickly, so you will use very few training epochs, in this case, just 2.\n",
    "\n",
    "There is a lot of data, so you will use a batch size of 128. After the model is trained, you will evaluate its accuracy on the test dataset.\n",
    "\n",
    "...\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "Tying all of this together, the complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54538f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               4000250   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 19s - loss: 0.4614 - accuracy: 0.7583 - val_loss: 0.3073 - val_accuracy: 0.8696 - 19s/epoch - 98ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 15s - loss: 0.1766 - accuracy: 0.9340 - val_loss: 0.3207 - val_accuracy: 0.8694 - 15s/epoch - 76ms/step\n",
      "Accuracy: 86.94%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3265f4ab",
   "metadata": {},
   "source": [
    "Running this example fits the model and summarizes the estimated performance.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "You can see that this very simple model achieves a score of 87%, which is in the neighborhood of the original paper, with minimal effort.\n",
    "\n",
    "Epoch 1/2\n",
    "196/196 - 19s - loss: 0.4614 - accuracy: 0.7583 - val_loss: 0.3073 - val_accuracy: 0.8696 - 19s/epoch - 98ms/step\n",
    "Epoch 2/2\n",
    "196/196 - 15s - loss: 0.1766 - accuracy: 0.9340 - val_loss: 0.3207 - val_accuracy: 0.8694 - 15s/epoch - 76ms/step\n",
    "Accuracy: 86.94%\n",
    "    \n",
    "You can likely do better if you trained this network, perhaps using a larger embedding and adding more hidden layers.\n",
    "\n",
    "Let’s try a different network type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56f9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           224000    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 250)               4000250   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,224,501\n",
      "Trainable params: 4,224,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 11s - loss: 0.5327 - accuracy: 0.6870 - val_loss: 0.3008 - val_accuracy: 0.8719 - 11s/epoch - 57ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 9s - loss: 0.1902 - accuracy: 0.9284 - val_loss: 0.3110 - val_accuracy: 0.8705 - 9s/epoch - 48ms/step\n",
      "Accuracy: 87.05%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem with larger embedding\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 7000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b498664",
   "metadata": {},
   "source": [
    "Changing the top_words from 5000 to 7000 did not appear to increase the performance of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6f254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution (importing modules) took: 0:00:00 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           224000    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1000)              16001000  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 250)               125250    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 125)               31375     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,882,251\n",
      "Trainable params: 16,882,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 35s - loss: 0.5245 - accuracy: 0.6885 - val_loss: 0.3153 - val_accuracy: 0.8662 - 35s/epoch - 179ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 38s - loss: 0.1836 - accuracy: 0.9308 - val_loss: 0.3186 - val_accuracy: 0.8637 - 38s/epoch - 194ms/step\n",
      "Accuracy: 86.37%\n",
      "Execution of the model took: 0:01:44 secs (Wall clock time)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# MLP for the IMDB problem with larger embedding and more hidden layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution (importing modules) took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 7000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(125, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution of the model took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(msg)  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6493a15e",
   "metadata": {},
   "source": [
    "Neither larger embedding and increasing hidden layer together helped improve the performance significantly above 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06790ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "One-Dimensional Convolutional Neural Network Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f73c5312",
   "metadata": {},
   "source": [
    "Convolutional neural networks were designed to honor the spatial structure in image data while being robust to the position and orientation of learned objects in the scene.\n",
    "\n",
    "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "\n",
    "Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes, respectively.\n",
    "\n",
    "Again, let’s import the classes and functions needed for this example and initialize your random number generator to a constant value so that you can easily reproduce the results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "791a8601",
   "metadata": {},
   "source": [
    "One-Dimensional Convolutional Neural Network Model for the IMDB Dataset\n",
    "Convolutional neural networks were designed to honor the spatial structure in image data while being robust to the position and orientation of learned objects in the scene.\n",
    "\n",
    "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "\n",
    "Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes, respectively.\n",
    "\n",
    "Again, let’s import the classes and functions needed for this example and initialize your random number generator to a constant value so that you can easily reproduce the results.\n",
    "\n",
    "# CNN for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "You can also load and prepare the IMDB dataset as you did before.\n",
    "\n",
    "...\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "You can now define your convolutional neural network model. This time, after the Embedding input layer, insert a Conv1D layer. This convolutional layer has 32 feature maps and reads embedded word representations’ three vector elements of the word embedding at a time.\n",
    "\n",
    "The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above.\n",
    "\n",
    "...\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "You will also fit the network the same as before.\n",
    "\n",
    "...\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "Tying all of this together, the complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5a7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 8000)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 250)               2000250   \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 29s - loss: 0.4962 - accuracy: 0.7173 - val_loss: 0.2827 - val_accuracy: 0.8816 - 29s/epoch - 147ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 20s - loss: 0.2310 - accuracy: 0.9084 - val_loss: 0.2723 - val_accuracy: 0.8877 - 20s/epoch - 100ms/step\n",
      "Accuracy: 88.77%\n",
      "\n",
      "\n",
      "\n",
      "Execution (importing modules) took: 0:01:19 secs (Wall clock time)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# CNN for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(msg)  \n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edit: Execution of the model took: 0:01:19 secs (Wall clock time)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "566c841a",
   "metadata": {},
   "source": [
    "One dimesnional convolutional model improved performance by 2% from 87% to 89%."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7f7060b",
   "metadata": {},
   "source": [
    "Running the example, you are first presented with a summary of the network structure. You can see your convolutional layer preserves the dimensionality of your Embedding input layer of 32-dimensional input with a maximum of 500 words. The pooling layer compresses this representation by halving it.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example offers a slight but welcome improvement over the neural network model above with an accuracy of 87%.\n",
    "\n",
    "Epoch 1/2\n",
    "196/196 - 29s - loss: 0.4962 - accuracy: 0.7173 - val_loss: 0.2827 - val_accuracy: 0.8816 - 29s/epoch - 147ms/step\n",
    "Epoch 2/2\n",
    "196/196 - 20s - loss: 0.2310 - accuracy: 0.9084 - val_loss: 0.2723 - val_accuracy: 0.8877 - 20s/epoch - 100ms/step\n",
    "Accuracy: 88.77%\n",
    "\n",
    "Again, there is a lot of opportunity for further optimization, such as using deeper and/or larger convolutional layers.\n",
    "\n",
    "One interesting idea is to set the max pooling layer to use an input length of 500. This would compress each feature map to a single 32-length vector and may boost performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
