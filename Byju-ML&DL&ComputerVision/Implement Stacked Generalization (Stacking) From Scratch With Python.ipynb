{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87db055",
   "metadata": {},
   "source": [
    "# How to Implement Stacked Generalization (Stacking) From Scratch With Python\n",
    "https://machinelearningmastery.com/implementing-stacking-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcfe6b",
   "metadata": {},
   "source": [
    "# Code a Stacking Ensemble From Scratch in Python, Step-by-Step."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a062c6bb",
   "metadata": {},
   "source": [
    "Ensemble methods are an excellent way to improve predictive performance on your machine learning problems.\n",
    "\n",
    "Stacked Generalization or stacking is an ensemble technique that uses a new model to learn how to best combine the predictions from two or more models trained on your dataset.\n",
    "\n",
    "In this tutorial, you will discover how to implement stacking from scratch in Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "How to learn to combine the predictions from multiple models on a dataset.\n",
    "How to apply stacked generalization to a real-world predictive modeling problem.\n",
    "Kick-start your project with my new book Machine Learning Algorithms From Scratch, including step-by-step tutorials and the Python source code files for all examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cbc60",
   "metadata": {},
   "source": [
    "# Description\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "203e979a",
   "metadata": {},
   "source": [
    "This section provides a brief overview of the Stacked Generalization algorithm and the Sonar dataset used in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked Generalization Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "986472b4",
   "metadata": {},
   "source": [
    "Stacked Generalization or stacking is an ensemble algorithm where a new model is trained to combine the predictions from two or more models already trained or your dataset.\n",
    "\n",
    "The predictions from the existing models or submodels are combined using a new model, and as such stacking is often referred to as blending, as the predictions from sub-models are blended together.\n",
    "\n",
    "It is typical to use a simple linear method to combine the predictions for submodels such as simple averaging or voting, to a weighted sum using linear regression or logistic regression.\n",
    "\n",
    "Models that have their predictions combined must have skill on the problem, but do not need to be the best possible models. This means that you do not need to tune the submodels intently, as long as the model shows some advantage over a baseline prediction.\n",
    "\n",
    "It is important that sub-models produce different predictions, so-called uncorrelated predictions. Stacking works best when the predictions that are combined are all skillful, but skillful in different ways. This may be achieved by using algorithms that use very different internal representations (trees compared to instances) and/or models trained on different representations or projections of the training data.\n",
    "\n",
    "In this tutorial, we will look at taking two very different and untuned sub-models and combining their predictions with a simple logistic regression algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b53c9a",
   "metadata": {},
   "source": [
    "# Sonar Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "448fe718",
   "metadata": {},
   "source": [
    "The dataset we will use in this tutorial is the Sonar dataset.\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
    "\n",
    "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
    "\n",
    "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of about 53%.\n",
    "\n",
    "You can learn more about this dataset at the UCI Machine Learning repository.\n",
    "\n",
    "Download the dataset for free and place it in your working directory with the filename sonar.all-data.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc2f75",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "917d91bb",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial is broken down into 3 steps:\n",
    "\n",
    "Sub-models and Aggregator.\n",
    "Combining Predictions.\n",
    "Sonar Dataset Case Study.\n",
    "These steps provide the foundation that you need to understand and implement stacking on your own predictive modeling problems.\n",
    "\n",
    "1. Sub-models and Aggregator\n",
    "We are going to use two models as submodels for stacking and a linear model as the aggregator model.\n",
    "\n",
    "This part is divided into 3 sections:\n",
    "\n",
    "Sub-model #1: k-Nearest Neighbors.\n",
    "Sub-model #2: Perceptron.\n",
    "Aggregator Model: Logistic Regression.\n",
    "Each model will be described in terms of the functions used to train the model and a function used to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab63c32",
   "metadata": {},
   "source": [
    "# 1.1 Sub-model #1: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356f050",
   "metadata": {},
   "source": [
    "# The k-Nearest Neighbors algorithm or kNN uses the entire training dataset as the model.\n",
    "\n",
    "Therefore training the model involves retaining the training dataset. Below is a function named knn_model() that does just this.\n",
    "\n",
    "# Prepare the kNN model\n",
    "def knn_model(train):\n",
    "\treturn train\n",
    "Making predictions involves finding the k most similar records in the training dataset and selecting the most common class values. The Euclidean distance function is used to calculate the similarity between new rows of data and rows in the training dataset.\n",
    "\n",
    "Below are these helper functions that involve making predictions for a kNN model. The function euclidean_distance() calculates the distance between two rows of data, get_neighbors() locates all neighbors for in the training dataset for a new row of data and knn_predict() makes a prediction from the neighbors for a new row of data.\n",
    "\n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate neighbors for a new row\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with kNN\n",
    "def knn_predict(model, test_row, num_neighbors=2):\n",
    "\tneighbors = get_neighbors(model, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    "You can see that the number of neighbors (k) is set to 2 as a default parameter on the knn_predict() function. This number was chosen with a little trial and error and was not tuned.\n",
    "\n",
    "Now that we have the building blocks for a kNN model, let’s look at the Perceptron algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f39bc7",
   "metadata": {},
   "source": [
    "# 1.2 Sub-model #2: Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129edd4",
   "metadata": {},
   "source": [
    "The model for the Perceptron algorithm is a set of weights learned from the training data.\n",
    "\n",
    "In order to train the weights, many predictions need to be made on the training data in order to calculate error values. Therefore, both model training and prediction require a function for prediction.\n",
    "\n",
    "Below are the helper functions for implementing the Perceptron algorithm. The perceptron_model() function trains the Perceptron model on the training dataset and perceptron_predict() is used to make a prediction for a row of data.\n",
    "\n",
    "# Make a prediction with weights\n",
    "def perceptron_predict(model, row):\n",
    "\tactivation = model[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += model[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    " \n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = perceptron_predict(weights, row)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    "The perceptron_model() model specifies both a learning rate and number of training epochs as default parameters. Again, these parameters were chosen with a little bit of trial and error, but were not tuned on the dataset.\n",
    "\n",
    "We now have implementations for both sub-models, let’s look at implementing the aggregator model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7ee59",
   "metadata": {},
   "source": [
    "# 1.3 Aggregator Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0b028c3",
   "metadata": {},
   "source": [
    "\n",
    "Like the Perceptron algorithm, Logistic Regression uses a set of weights, called coefficients, as the representation of the model.\n",
    "\n",
    "And like the Perceptron algorithm, the coefficients are learned by iteratively making predictions on the training data and updating them.\n",
    "\n",
    "Below are the helper functions for implementing the logistic regression algorithm. The logistic_regression_model() function is used to train the coefficients on the training dataset and logistic_regression_predict() is used to make a prediction for a row of data.\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def logistic_regression_predict(model, row):\n",
    "\tyhat = model[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += model[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = logistic_regression_predict(coef, row)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\treturn coef\n",
    "The logistic_regression_model() defines a learning rate and number of epochs as default parameters, and as with the other algorithms, these parameters were found with a little trial and error and were not optimized.\n",
    "\n",
    "Now that we have implementations of sub-models and the aggregator model, let’s see how we can combine the predictions from multiple models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f4a0f",
   "metadata": {},
   "source": [
    "# 2. Combining Predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "005b55c3",
   "metadata": {},
   "source": [
    "For a machine learning algorithm, learning how to combine predictions is much the same as learning from a training dataset.\n",
    "\n",
    "A new training dataset can be constructed from the predictions of the sub-models, as follows:\n",
    "\n",
    "Each row represents one row in the training dataset.\n",
    "The first column contains predictions for each row in the training dataset made by the first sub-model, such as k-Nearest Neighbors.\n",
    "The second column contains predictions for each row in the training dataset made by the second sub-model, such as the Perceptron algorithm.\n",
    "The third column contains the expected output value for the row in the training dataset.\n",
    "Below is a contrived example of what a constructed stacking dataset may look like:\n",
    "\n",
    "KNN,\tPer,\tY\n",
    "0,\t0\t0\n",
    "1,\t0\t1\n",
    "0,\t1\t0\n",
    "1,\t1\t1\n",
    "0,\t1\t0\n",
    "A machine learning algorithm, such as logistic regression can then be trained on this new dataset. In essence, this new meta-algorithm learns how to best combine the prediction from multiple submodels.\n",
    "\n",
    "Below is a function named to_stacked_row() that implements this procedure for creating new rows for this stacked dataset.\n",
    "\n",
    "The function takes a list of models as input, these are used to make predictions. The function also takes a list of functions as input, one function used to make a prediction for each model. Finally, a single row from the training dataset is included.\n",
    "\n",
    "A new row is constructed one column at a time. Predictions are calculated using each model and the row of training data. The expected output value from the training dataset row is then added as the last column to the row.\n",
    "\n",
    "# Make predictions with sub-models and construct a new stacked row\n",
    "def to_stacked_row(models, predict_list, row):\n",
    "\tstacked_row = list()\n",
    "\tfor i in range(len(models)):\n",
    "\t\tprediction = predict_list[i](models[i], row)\n",
    "\t\tstacked_row.append(prediction)\n",
    "\tstacked_row.append(row[-1])\n",
    "\treturn stacked_row\n",
    "On some predictive modeling problems, it is possible to get an even larger boost by training the aggregated model on both the training row and the predictions made by sub-models.\n",
    "\n",
    "This improvement gives the aggregator model both the context of all the data in the training row to help determine how and when to best combine the predictions of the sub-models.\n",
    "\n",
    "We can update our to_stacked_row() function to include this by aggregating the training row (minus the final column) and the stacked row as created above.\n",
    "\n",
    "Below is an updated version of the to_stacked_row() function that implements this improvement.\n",
    "\n",
    "# Make predictions with sub-models and construct a new stacked row\n",
    "def to_stacked_row(models, predict_list, row):\n",
    "\tstacked_row = list()\n",
    "\tfor i in range(len(models)):\n",
    "\t\tprediction = predict_list[i](models[i], row)\n",
    "\t\tstacked_row.append(prediction)\n",
    "\tstacked_row.append(row[-1])\n",
    "\treturn row[0:len(row)-1] + stacked_row\n",
    "It is a good idea to try both approaches on your problem to see which works best.\n",
    "\n",
    "Now that we have all of the pieces for stacked generalization, we can apply it to a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b857da",
   "metadata": {},
   "source": [
    "# 3. Sonar Dataset Case Study"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edd3566c",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we will apply the Stacking algorithm to the Sonar dataset.\n",
    "\n",
    "The example assumes that a CSV copy of the dataset is in the current working directory with the filename sonar.all-data.csv.\n",
    "\n",
    "The dataset is first loaded, the string values converted to numeric and the output column is converted from strings to the integer values of 0 to 1. This is achieved with helper functions load_csv(), str_column_to_float() and str_column_to_int() to load and prepare the dataset.\n",
    "\n",
    "We will use k-fold cross validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model error. Classification accuracy will be used to evaluate the model. These behaviors are provided in the cross_validation_split(), accuracy_metric() and evaluate_algorithm() helper functions.\n",
    "\n",
    "We will use the k-Nearest Neighbors, Perceptron and Logistic Regression algorithms implemented above. We will also use our technique for creating the new stacked dataset defined in the previous step.\n",
    "\n",
    "A new function name stacking() is developed. This function does 4 things:\n",
    "\n",
    "It first trains a list of models (kNN and Perceptron).\n",
    "It then uses the models to make predictions and create a new stacked dataset.\n",
    "It then trains an aggregator model (logistic regression) on the stacked dataset.\n",
    "It then uses the sub-models and aggregator model to make predictions on the test dataset.\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c8dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [78.26086956521739, 76.81159420289855, 69.56521739130434]\n",
      "Mean Accuracy: 74.879%\n"
     ]
    }
   ],
   "source": [
    "# Test stacking on the sonar dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate neighbors for a new row\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with kNN\n",
    "def knn_predict(model, test_row, num_neighbors=2):\n",
    "\tneighbors = get_neighbors(model, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# Prepare the kNN model\n",
    "def knn_model(train):\n",
    "\treturn train\n",
    " \n",
    "# Make a prediction with weights\n",
    "def perceptron_predict(model, row):\n",
    "\tactivation = model[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += model[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    " \n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = perceptron_predict(weights, row)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def logistic_regression_predict(model, row):\n",
    "\tyhat = model[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += model[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = logistic_regression_predict(coef, row)\n",
    "\t\t\terror = row[-1] - yhat\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\treturn coef\n",
    " \n",
    "# Make predictions with sub-models and construct a new stacked row\n",
    "def to_stacked_row(models, predict_list, row):\n",
    "\tstacked_row = list()\n",
    "\tfor i in range(len(models)):\n",
    "\t\tprediction = predict_list[i](models[i], row)\n",
    "\t\tstacked_row.append(prediction)\n",
    "\tstacked_row.append(row[-1])\n",
    "\treturn row[0:len(row)-1] + stacked_row\n",
    " \n",
    "# Stacked Generalization Algorithm\n",
    "def stacking(train, test):\n",
    "\tmodel_list = [knn_model, perceptron_model]\n",
    "\tpredict_list = [knn_predict, perceptron_predict]\n",
    "\tmodels = list()\n",
    "\tfor i in range(len(model_list)):\n",
    "\t\tmodel = model_list[i](train)\n",
    "\t\tmodels.append(model)\n",
    "\tstacked_dataset = list()\n",
    "\tfor row in train:\n",
    "\t\tstacked_row = to_stacked_row(models, predict_list, row)\n",
    "\t\tstacked_dataset.append(stacked_row)\n",
    "\tstacked_model = logistic_regression_model(stacked_dataset)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tstacked_row = to_stacked_row(models, predict_list, row)\n",
    "\t\tstacked_dataset.append(stacked_row)\n",
    "\t\tprediction = logistic_regression_predict(stacked_model, stacked_row)\n",
    "\t\tprediction = round(prediction)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn predictions\n",
    " \n",
    "# Test stacking on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "n_folds = 3\n",
    "scores = evaluate_algorithm(dataset, stacking, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1ec2663",
   "metadata": {},
   "source": [
    "A k value of 3 was used for cross-validation, giving each fold 208/3 = 69.3 or just under 70 records to be evaluated upon each iteration.\n",
    "\n",
    "Running the example prints the scores and mean of the scores for the final configuration.\n",
    "\n",
    "Scores: [78.26086956521739, 76.81159420289855, 69.56521739130434]\n",
    "Mean Accuracy: 74.879%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632a80b",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85545faa",
   "metadata": {},
   "source": [
    "\n",
    "This section lists extensions to this tutorial that you may be interested in exploring.\n",
    "\n",
    "Tune Algorithms. The algorithms used for the submodels and the aggregate model in this tutorial were not tuned. Explore alternate configurations and see if you can further lift performance.\n",
    "Prediction Correlations. Stacking works better if the predictions of submodels are weakly correlated. Implement calculations to estimate the correlation between the predictions of submodels.\n",
    "Different Sub-models. Implement more and different sub-models to be combined using the stacking procedure.\n",
    "Different Aggregating Model. Experiment with simpler models (like averaging and voting) and more complex aggregation models to see if you can boost performance.\n",
    "More Datasets. Apply stacking to more datasets on the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ab207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
