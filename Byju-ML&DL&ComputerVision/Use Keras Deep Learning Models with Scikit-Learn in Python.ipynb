{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7aef83",
   "metadata": {},
   "source": [
    "# Use Keras Deep Learning Models with Scikit-Learn in Python\n",
    "https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22243b89",
   "metadata": {},
   "source": [
    "Keras is one of the most popular deep learning libraries in Python for research and development because of its simplicity and ease of use.\n",
    "\n",
    "The scikit-learn library is the most popular library for general machine learning in Python.\n",
    "\n",
    "In this post, you will discover how you can use deep learning models from Keras with the scikit-learn library in Python.\n",
    "\n",
    "This will allow you to leverage the power of the scikit-learn library for tasks like model evaluation and model hyper-parameter optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ed38f",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10679dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keras is a popular library for deep learning in Python, but the focus of the library is deep learning models. In fact, it strives for minimalism, focusing on only what you need to quickly and simply define and build deep learning models.\n",
    "\n",
    "The scikit-learn library in Python is built upon the SciPy stack for efficient numerical computation. It is a fully featured library for general machine learning and provides many useful utilities in developing deep learning models. Not least of which are:\n",
    "\n",
    "Evaluation of models using resampling methods like k-fold cross validation\n",
    "Efficient search and evaluation of model hyper-parameters\n",
    "There was a wrapper in the TensorFlow/Keras library to make deep learning models used as classification or regression estimators in scikit-learn. But recently, this wrapper was taken out to become a standalone Python module.\n",
    "\n",
    "In the following sections, you will work through examples of using the KerasClassifier wrapper for a classification neural network created in Keras and used in the scikit-learn library.\n",
    "\n",
    "The test problem is the Pima Indians onset of diabetes classification dataset. This is a small dataset with all numerical attributes that is easy to work with. Download the dataset and place it in your currently working directly with the name pima-indians-diabetes.csv (update: download from here).\n",
    "\n",
    "The following examples assume you have successfully installed TensorFlow 2.x, SciKeras, and scikit-learn. If you use the pip system for your Python modules, you may install them with:\n",
    "\n",
    "!pip install tensorflow scikeras scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0856b22",
   "metadata": {},
   "source": [
    "# Evaluate Deep Learning Models with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205ef99",
   "metadata": {},
   "source": [
    "# Manual k-Fold Cross Validation # (From the book Deeplearning with Python; Page 64-65 of 255)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0499ed1b",
   "metadata": {},
   "source": [
    "The gold standard for machine learning model evaluation is k-fold cross validation. It provides\n",
    "a robust estimate of the performance of a model on unseen data. It does this by splitting the\n",
    "training dataset into k subsets and takes turns training models on all subsets except one which\n",
    "is held out, and evaluating model performance on the held out validation dataset. The process\n",
    "is repeated until all subsets are given an opportunity to be the held out validation set. The\n",
    "performance measure is then averaged across all models that are created.\n",
    "Cross validation is often not used for evaluating deep learning models because of the greater\n",
    "computational expense. For example k-fold cross validation is often used with 5 or 10 folds. As\n",
    "such, 5 or 10 models must be constructed and evaluated, greatly adding to the evaluation time\n",
    "of a model. Nevertheless, when the problem is small enough or if you have sufficient compute\n",
    "resources, k-fold cross validation can give you a less biased estimate of the performance of your\n",
    "model.\n",
    "In the example below we use the handy StratifiedKFold class1 from the scikit-learn Python\n",
    "machine learning library to split up the training dataset into 10 folds. The folds are stratified,\n",
    "meaning that the algorithm attempts to balance the number of instances of each class in each\n",
    "fold. The example creates and evaluates 10 models using the 10 splits of the data and collects\n",
    "all of the scores. The verbose output for each epoch is turned oâ†µ by passing verbose=0 to the\n",
    "fit() and evaluate() functions on the model. The performance is printed for each model and\n",
    "it is stored. The average and standard deviation of the model performance is then printed at\n",
    "the end of the run to provide a robust estimate of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63767450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-788f78a51f96>:3: UserWarning: DelftStack\n",
      "  warnings.warn('DelftStack')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.32%\n",
      "accuracy: 75.32%\n",
      "accuracy: 79.22%\n",
      "accuracy: 75.32%\n",
      "accuracy: 68.83%\n",
      "accuracy: 68.83%\n",
      "accuracy: 63.64%\n",
      "accuracy: 66.23%\n",
      "accuracy: 80.26%\n",
      "accuracy: 77.63%\n",
      "73.06% (+/- 5.45%)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*do not.*', )\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "\n",
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu' ))\n",
    "    model.add(Dense(1, activation='sigmoid' ))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6098dad4",
   "metadata": {},
   "source": [
    "Notice that we had to re-create the model each loop to then fit and evaluate it with the data\n",
    "for the fold. In the next lesson we will look at how we can use Keras models natively with the\n",
    "scikit-learn machine learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54314084",
   "metadata": {},
   "source": [
    "# back to MachineLearningMastery.com\n",
    "https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10550c17",
   "metadata": {},
   "source": [
    "The KerasClassifier and KerasRegressor classes in SciKeras take an argument model which is the name of the function to call to get your model.\n",
    "\n",
    "You must define a function called whatever you like that defines your model, compiles it, and returns it.\n",
    "\n",
    "In the example below, you will define a function create_model() that creates a simple multi-layer neural network for the problem.\n",
    "\n",
    "You pass this function name to the KerasClassifier class by the model argument. You also pass in additional arguments of nb_epoch=150 and batch_size=10. These are automatically bundled up and passed on to the fit() function, which is called internally by the KerasClassifier class.\n",
    "\n",
    "In this example, you will use the scikit-learn StratifiedKFold to perform 10-fold stratified cross-validation. This is a resampling technique that can provide a robust estimate of the performance of a machine learning model on unseen data.\n",
    "\n",
    "Next, use the scikit-learn function cross_val_score() to evaluate your model using the cross-validation scheme and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4619693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7331510594668489\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    " \n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d6d1a98",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example displays the skill of the model for each epoch. A total of 10 models are created and evaluated, and the final average accuracy is displayed.\n",
    "\n",
    "0.7331510594668489"
   ]
  },
  {
   "cell_type": "raw",
   "id": "879cf10b",
   "metadata": {},
   "source": [
    "In comparison, the following is an equivalent implementation with a neural network model in scikit-learn:\n",
    "The role of the KerasClassifier is to work as an adapter to make the Keras model work like a MLPClassifier object from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522c3008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-9c32bb602f66>:3: UserWarning: DelftStack\n",
      "  warnings.warn('DelftStack')\n",
      "C:\\Users\\Byju\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7123547505126452\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*do not.*', )\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "\n",
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = MLPClassifier(hidden_layer_sizes=(12,8), activation='relu', max_iter=150, batch_size=10, verbose=False)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5d2c9",
   "metadata": {},
   "source": [
    "# Grid Search Deep Learning Model Parameters\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79e01239",
   "metadata": {},
   "source": [
    "\n",
    "The previous example showed how easy it is to wrap your deep learning model from Keras and use it in functions from the scikit-learn library.\n",
    "\n",
    "In this example, you will go a step further. The function that you specify to the model argument when creating the KerasClassifier wrapper can take arguments. You can use these arguments to further customize the construction of the model. In addition, you know you can provide arguments to the fit() function.\n",
    "\n",
    "In this example, you will use a grid search to evaluate different configurations for your neural network model and report on the combination that provides the best-estimated performance.\n",
    "\n",
    "The create_model() function is defined to take two arguments, optimizer and init, both of which must have default values. This will allow you to evaluate the effect of using different optimization algorithms and weight initialization schemes for your network.\n",
    "\n",
    "After creating your model, define the arrays of values for the parameter you wish to search, specifically:\n",
    "\n",
    "Optimizers for searching different weight values\n",
    "Initializers for preparing the network weights using different schemes\n",
    "Epochs for training the model for a different number of exposures to the training dataset\n",
    "Batches for varying the number of samples before a weight update\n",
    "\n",
    "The options are specified into a dictionary and passed to the configuration of the GridSearchCV scikit-learn class. This class will evaluate a version of your neural network model for each combination of parameters (2 x 3 x 3 x 3 for the combinations of optimizers, initializations, epochs, and batches). Each combination is then evaluated using the default of 3-fold stratified cross validation.\n",
    "\n",
    "That is a lot of models and a lot of computation. This is not a scheme you want to use lightly because of the time it will take. It may be useful for you to design small experiments with a smaller subset of your data that will complete in a reasonable time. This is reasonable in this case because of the small network and the small dataset (less than 1000 instances and nine attributes).\n",
    "\n",
    "Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af05e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'class_weight'])\n",
      "Best: 0.756549 using {'batch_size': 5, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.691342 (0.044405) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.708302 (0.028622) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.714837 (0.027726) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.743545 (0.018841) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.725303 (0.036374) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.726560 (0.014253) with: {'batch_size': 5, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.733130 (0.033545) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.714846 (0.045447) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.739640 (0.018074) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.742238 (0.020599) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.750064 (0.026891) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.756549 (0.019081) with: {'batch_size': 5, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.734445 (0.032737) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.720083 (0.031974) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.752636 (0.024744) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.755250 (0.017158) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.748706 (0.011869) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.749996 (0.006738) with: {'batch_size': 5, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.664018 (0.021485) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.674450 (0.009257) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.740879 (0.005127) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.716145 (0.022021) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.721356 (0.020714) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.699287 (0.030785) with: {'batch_size': 10, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.703124 (0.015775) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.699321 (0.033669) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.730490 (0.014756) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.736941 (0.026898) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.730524 (0.027262) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.738299 (0.027641) with: {'batch_size': 10, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.703175 (0.063798) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.673347 (0.102685) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.738333 (0.030151) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.748739 (0.027711) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.740922 (0.026084) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.722825 (0.072816) with: {'batch_size': 10, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.667974 (0.027823) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.661531 (0.048653) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.712308 (0.024386) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.709600 (0.028514) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.705738 (0.012353) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.677124 (0.034040) with: {'batch_size': 20, 'epochs': 50, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.697920 (0.026467) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.692700 (0.030658) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.730447 (0.025201) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.714956 (0.045428) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.731814 (0.018440) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.714803 (0.020491) with: {'batch_size': 20, 'epochs': 100, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.705764 (0.030329) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.716162 (0.046004) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.744852 (0.028370) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.744801 (0.022210) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.746125 (0.023275) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.736966 (0.014854) with: {'batch_size': 20, 'epochs': 150, 'model__init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow scikeras scikit-learn\n",
    "\n",
    "# MLP for Pima Indians Dataset with grid search via sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    " \n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "print(model.get_params().keys())\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, model__init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19c2ad",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b420493",
   "metadata": {},
   "source": [
    "In this post, you discovered how to wrap your Keras deep learning models and use them in the scikit-learn general machine learning library.\n",
    "\n",
    "You can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save a lot of time over implementing these schemes yourself.\n",
    "\n",
    "Wrapping your model allowed you to leverage powerful tools from scikit-learn to fit your deep learning models into your general machine learning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
