{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccdf1fe",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size\n",
    "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98512db9",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is the dominant method used to train deep learning models.\n",
    "There are three main variants of gradient descent and it can be confusing which one to use.\n",
    "In this post, you will discover the one type of gradient descent you should use in general and how to configure it.\n",
    "After completing this post, you will know:\n",
    "•\tWhat gradient descent is and how it works from a high level.\n",
    "•\tWhat batch, stochastic, and mini-batch gradient descent are and the benefits and limitations of each method.\n",
    "•\tThat mini-batch gradient descent is the go-to method and how to configure it on your applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae482e63",
   "metadata": {},
   "source": [
    "# Tutorial Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8920d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "This tutorial is divided into 3 parts; they are:\n",
    "1.\tWhat is Gradient Descent?\n",
    "2.\tContrasting the 3 Types of Gradient Descent\n",
    "3.\tHow to Configure Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce933500",
   "metadata": {},
   "source": [
    "# What is Gradient Descent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8e1dd07",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm often used for finding the weights or coefficients of machine learning algorithms, such as artificial neural networks and logistic regression.\n",
    "It works by having the model make predictions on training data and using the error on the predictions to update the model in such a way as to reduce the error.\n",
    "The goal of the algorithm is to find model parameters (e.g. coefficients or weights) that minimize the error of the model on the training dataset. It does this by making changes to the model that move it along a gradient or slope of errors down toward a minimum error value. This gives the algorithm its name of “gradient descent.”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87cd53f9",
   "metadata": {},
   "source": [
    "The pseudocode sketch below summarizes the gradient descent algorithm:\n",
    "    \n",
    "model = initialization(...)\n",
    "n_epochs = ...\n",
    "train_data = ...\n",
    "for i in n_epochs:\n",
    "\ttrain_data = shuffle(train_data)\n",
    "\tX, y = split(train_data)\n",
    "\tpredictions = predict(X, model)\n",
    "\terror = calculate_error(y, predictions)\n",
    "\tmodel = update_model(model, error)\n",
    "\n",
    "Neural networks are trained using the gradient descent optimization algorithm. This involves using the current state of the model to make a prediction, comparing the prediction to the expected values, and using the difference as an estimate of the error gradient. This error gradient is then used to update the model weights and the process is repeated until it minimizes the error of the model on the training dataset.It does this by making changes to the model that move it along a gradient or slope of errors down toward a minimum error value. This gives the algorithm its name of “gradient descent.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Contrasting the 3 Types of Gradient Descent\n",
    "Gradient descent can vary in terms of the number of training patterns used to calculate error; that is in turn used to update the model.\n",
    "The number of patterns used to calculate the error includes how stable the gradient is that is used to update the model. We will see that there is a tension in gradient descent configurations of computational efficiency and the fidelity of the error gradient.\n",
    "The three main flavors of gradient descent are batch, stochastic, and mini-batch.\n",
    "Let’s take a closer look at each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is Stochastic Gradient Descent?\n",
    "Stochastic gradient descent, often abbreviated SGD, is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset.\n",
    "The update of the model for each training example means that stochastic gradient descent is often called an online machine learning algorithm.\n",
    "Upsides\n",
    "•\tThe frequent updates immediately give an insight into the performance of the model and the rate of improvement.\n",
    "•\tThis variant of gradient descent may be the simplest to understand and implement, especially for beginners.\n",
    "•\tThe increased model update frequency can result in faster learning on some problems.\n",
    "•\tThe noisy update process can allow the model to avoid local minima (e.g. premature convergence).\n",
    "Downsides\n",
    "•\tUpdating the model so frequently is more computationally expensive than other configurations of gradient descent, taking significantly longer to train models on large datasets.\n",
    "•\tThe frequent updates can result in a noisy gradient signal, which may cause the model parameters and in turn the model error to jump around (have a higher variance over training epochs).\n",
    "•\tThe noisy learning process down the error gradient can also make it hard for the algorithm to settle on an error minimum for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is Batch Gradient Descent?\n",
    "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated.\n",
    "One cycle through the entire training dataset is called a training epoch. Therefore, it is often said that batch gradient descent performs model updates at the end of each training epoch.\n",
    "Upsides\n",
    "•\tFewer updates to the model means this variant of gradient descent is more computationally efficient than stochastic gradient descent.\n",
    "•\tThe decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems.\n",
    "•\tThe separation of the calculation of prediction errors and the model update lends the algorithm to parallel processing based implementations.\n",
    "Downsides\n",
    "•\tThe more stable error gradient may result in premature convergence of the model to a less optimal set of parameters.\n",
    "•\tThe updates at the end of the training epoch require the additional complexity of accumulating prediction errors across all training examples.\n",
    "•\tCommonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.\n",
    "•\tModel updates, and in turn training speed, may become very slow for large datasets."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3e1a11d",
   "metadata": {},
   "source": [
    "How to Configure Mini-Batch Gradient Descent\n",
    "Mini-batch gradient descent is the recommended variant of gradient descent for most applications, especially in deep learning.\n",
    "Mini-batch sizes, commonly called “batch sizes” for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed. Such as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on.\n",
    "Batch size is a slider on the learning process.\n",
    "•\tSmall values give a learning process that converges quickly at the cost of noise in the training process.\n",
    "•\tLarge values give a learning process that converges slowly with accurate estimates of the error gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b47fe5",
   "metadata": {},
   "source": [
    "# Tip 1: A good default for batch size might be 32."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88822dcf",
   "metadata": {},
   "source": [
    "… [batch size] is typically chosen between 1 and a few hundreds, e.g. [batch size] = 32 is a good default value, with values above 10 taking advantage of the speedup of matrix-matrix products over matrix-vector products.\n",
    "— Practical recommendations for gradient-based training of deep architectures, 2012\n",
    "Update 2018: here is another paper supporting a batch size of 32, here’s the quote (m is batch size):\n",
    "The presented results confirm that using small batch sizes achieves the best training stability and generalization performance, for a given computational cost, across a wide range of experiments. In all cases the best results have been obtained with batch sizes m = 32 or smaller, often as small as m = 2 or m = 4.\n",
    "— Revisiting Small Batch Training for Deep Neural Networks, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5425d8",
   "metadata": {},
   "source": [
    "# Tip 2: It is a good idea to review learning curves of model validation error against training time with different batch sizes when tuning the batch size."
   ]
  },
  {
   "cell_type": "raw",
   "id": "03d0c8dc",
   "metadata": {},
   "source": [
    "… it can be optimized separately of the other hyperparameters, by comparing training curves (training and validation error vs amount of training time), after the other hyper-parameters (except learning rate) have been selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0bc89",
   "metadata": {},
   "source": [
    "# Tip 3: Tune batch size and learning rate after tuning all other hyperparameters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a23d067c",
   "metadata": {},
   "source": [
    "… [batch size] and [learning rate] may slightly interact with other hyper-parameters so both should be re-optimized at the end. Once [batch size] is selected, it can generally be fixed while the other hyper-parameters can be further optimized (except for a momentum hyper-parameter, if one is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75146ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a692d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2d708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
