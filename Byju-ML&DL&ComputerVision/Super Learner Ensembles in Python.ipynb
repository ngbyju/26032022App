{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a429b6",
   "metadata": {},
   "source": [
    "# Super Learner Ensembles in Python\n",
    "https://machinelearningmastery.com/super-learner-ensemble-in-python/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "655b0c69",
   "metadata": {},
   "source": [
    "Selecting a machine learning algorithm for a predictive modeling problem involves evaluating many different models and model configurations using k-fold cross-validation.\n",
    "\n",
    "The super learner is an ensemble machine learning algorithm that combines all of the models and model configurations that you might investigate for a predictive modeling problem and uses them to make a prediction as-good-as or better than any single model that you may have investigated.\n",
    "\n",
    "The super learner algorithm is an application of stacked generalization, called stacking or blending, to k-fold cross-validation where all models use the same k-fold splits of the data and a meta-model is fit on the out-of-fold predictions from each model.\n",
    "\n",
    "In this tutorial, you will discover the super learner ensemble machine learning algorithm.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "Super learner is the application of stacked generalization using out-of-fold predictions during k-fold cross-validation.\n",
    "The super learner ensemble algorithm is straightforward to implement in Python using scikit-learn models.\n",
    "The ML-Ensemble (mlens) library provides a convenient implementation that allows the super learner to be fit and used in just a few lines of code.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65404fb5",
   "metadata": {},
   "source": [
    "Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "What Is the Super Learner?\n",
    "Manually Develop a Super Learner With scikit-learn\n",
    "Super Learner With ML-Ensemble Library (mlens)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "367f3c7d",
   "metadata": {},
   "source": [
    "What Is the Super Learner?\n",
    "There are many hundreds of models to choose from for a predictive modeling problem; which one is best?\n",
    "\n",
    "Then, after a model is chosen, how do you best configure it for your specific dataset?\n",
    "\n",
    "These are open questions in applied machine learning. The best answer we have at the moment is to use empirical experimentation to test and discover what works best for your dataset.\n",
    "\n",
    "In practice, it is generally impossible to know a priori which learner will perform best for a given prediction problem and data set.\n",
    "\n",
    "— Super Learner, 2007.\n",
    "\n",
    "This involves selecting many different algorithms that may be appropriate for your regression or classification problem and evaluating their performance on your dataset using a resampling technique, such as k-fold cross-validation.\n",
    "\n",
    "The algorithm that performs the best on your dataset according to k-fold cross-validation is then selected, fit on all available data, and you can then start using it to make predictions.\n",
    "\n",
    "There is an alternative approach.\n",
    "\n",
    "Consider that you have already fit many different algorithms on your dataset, and some algorithms have been evaluated many times with different configurations. You may have many tens or hundreds of different models of your problem. Why not use all those models instead of the best model from the group?\n",
    "\n",
    "This is the intuition behind the so-called “super learner” ensemble algorithm.\n",
    "\n",
    "The super learner algorithm involves first pre-defining the k-fold split of your data, then evaluating all different algorithms and algorithm configurations on the same split of the data. All out-of-fold predictions are then kept and used to train an algorithm that learns how to best combine the predictions.\n",
    "\n",
    "The algorithms may differ in the subset of the covariates used, the basis functions, the loss functions, the searching algorithm, and the range of tuning parameters, among others.\n",
    "\n",
    "— Super Learner In Prediction, 2010.\n",
    "\n",
    "The results of this model should be no worse than the best performing model evaluated during k-fold cross-validation and has the likelihood of performing better than any single model.\n",
    "\n",
    "The super learner algorithm was proposed by Mark van der Laan, Eric Polley, and Alan Hubbard from Berkeley in their 2007 paper titled “Super Learner.” It was published in a biological journal, which may be sheltered from the broader machine learning community.\n",
    "\n",
    "The super learner technique is an example of the general method called “stacked generalization,” or “stacking” for short, and is known in applied machine learning as blending, as often a linear model is used as the meta-model.\n",
    "\n",
    "The super learner is related to the stacking algorithm introduced in neural networks context …\n",
    "\n",
    "— Super Learner In Prediction, 2010.\n",
    "\n",
    "For more on the topic stacking, see the posts:\n",
    "\n",
    "How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras\n",
    "https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "How to Implement Stacked Generalization (Stacking) From Scratch With Python\n",
    "We can think of the “super learner” as a specific configuration of stacking specifically to k-fold cross-validation.\n",
    "\n",
    "I have sometimes seen this type of blending ensemble referred to as a cross-validation ensemble.\n",
    "\n",
    "The procedure can be summarized as follows:\n",
    "\n",
    "1. Select a k-fold split of the training dataset.\n",
    "2. Select m base-models or model configurations.\n",
    "3. For each basemodel:\n",
    "a. Evaluate using k-fold cross-validation.\n",
    "b. Store all out-of-fold predictions.\n",
    "c. Fit the model on the full training dataset and store.\n",
    "4. Fit a meta-model on the out-of-fold predictions.\n",
    "5. Evaluate the model on a holdout dataset or use model to make predictions.\n",
    "The image below, taken from the original paper, summarizes this data flow.\n",
    "\n",
    "Diagram Showing the Data Flow of the Super Learner Algorithm\n",
    "Diagram Showing the Data Flow of the Super Learner Algorithm\n",
    "Taken from “Super Learner.”\n",
    "\n",
    "Let’s take a closer look at some common sticking points you may have with this procedure.\n",
    "\n",
    "Q. What are the inputs and outputs for the meta-model?\n",
    "\n",
    "The meta-model takes in predictions from base-models as input and predicts the target for the training dataset as output:\n",
    "\n",
    "Input: Predictions from base-models.\n",
    "Output: Prediction for training dataset.\n",
    "For example, if we had 50 base-models, then one input sample would be a vector with 50 values, each value in the vector representing a prediction from one of the base-models for one sample of the training dataset.\n",
    "\n",
    "If we had 1,000 examples (rows) in the training dataset and 50 models, then the input data for the meta-model would be 1,000 rows and 50 columns.\n",
    "\n",
    "Q. Won’t the meta-model overfit the training data?\n",
    "\n",
    "Probably not.\n",
    "\n",
    "This is the trick of the super learner, and the stacked generalization procedure in general.\n",
    "\n",
    "The input to the meta-model is the out-of-fold (out-of-sample) predictions. In aggregate, the out-of-fold predictions for a model represent the model’s skill or capability in making predictions on data not seen during training.\n",
    "\n",
    "By training a meta-model on out-of-sample predictions of other models, the meta-model learns how to both correct the out-of-sample predictions for each model and to best combine the out-of-sample predictions from multiple models; actually, it does both tasks at the same time.\n",
    "\n",
    "Importantly, to get an idea of the true capability of the meta-model, it must be evaluated on new out-of-sample data. That is, data not used to train the base models.\n",
    "\n",
    "Q. Can this work for regression and classification?\n",
    "\n",
    "Yes, it was described in the papers for regression (predicting a numerical value).\n",
    "\n",
    "It can work just as well for classification (predicting a class label), although it is probably best to predict probabilities to give the meta-model more granularity when combining predictions.\n",
    "\n",
    "Q. Why do we fit each base-model on the entire training dataset?\n",
    "\n",
    "Each base-model is fit on the entire training dataset so that the model can be used later to make predictions on new examples not seen during training.\n",
    "\n",
    "This step is strictly not required until predictions are needed by the super learner.\n",
    "\n",
    "Q. How do we make a prediction?\n",
    "\n",
    "To make a prediction on a new sample (row of data), first, the row of data is provided as input to each base model to generate a prediction from each model.\n",
    "\n",
    "The predictions from the base-models are then concatenated into a vector and provided as input to the meta-model. The meta-model then makes a final prediction for the row of data.\n",
    "\n",
    "We can summarize this procedure as follows:\n",
    "\n",
    "1. Take a sample not seen by the models during training.\n",
    "2. For each base-model:\n",
    "a. Make a prediction given the sample.\n",
    "b. Store prediction.\n",
    "3. Concatenate predictions from submodel into a single vector.\n",
    "4. Provide vector as input to the meta-model to make a final prediction.\n",
    "Now that we are familiar with the super learner algorithm, let’s look at a worked example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f0704",
   "metadata": {},
   "source": [
    "# Manually Develop a Super Learner With scikit-learn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38d34589",
   "metadata": {},
   "source": [
    "\n",
    "The Super Learner algorithm is relatively straightforward to implement on top of the scikit-learn Python machine learning library.\n",
    "\n",
    "In this section, we will develop an example of super learning for both regression and classification that you can adapt to your own problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72397e",
   "metadata": {},
   "source": [
    "# Super Learner for Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "752f8766",
   "metadata": {},
   "source": [
    "\n",
    "We will use the make_regression() test problem and generate 1,000 examples (rows) with 100 features (columns). This is a simple regression problem with a linear relationship between input and output, with added noise.\n",
    "\n",
    "We will split the data so that 50 percent is used for training the model and 50 percent is held back to evaluate the final super model and base-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef520cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) \n",
      "Test (500, 100) (500,)\n"
     ]
    }
   ],
   "source": [
    "# create the inputs and outputs\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, '\\nTest', X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1588afc",
   "metadata": {},
   "source": [
    "Next, we will define a bunch of different regression models.\n",
    "\n",
    "In this case, we will use nine different algorithms with modest configuration. You can use any models or model configurations you like.\n",
    "\n",
    "The get_models() function below defines all of the models and returns them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f1c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # for building linear Regression model\n",
    "from sklearn.linear_model import ElasticNet # for building Elasticnet model\n",
    "from sklearn.svm import SVR # for building SVR model\n",
    "from sklearn.tree import DecisionTreeRegressor  # for building Decision Tree Regressor model\n",
    "from sklearn.neighbors import KNeighborsRegressor # for building K Nearest Neighbors Regressor model\n",
    "from sklearn.ensemble import AdaBoostRegressor # for building Adaboost Regressor model\n",
    "from sklearn.ensemble import BaggingRegressor # for building Bagging Regressor model\n",
    "from sklearn.ensemble import RandomForestRegressor # for building RandomForest Regressor model\n",
    "from sklearn.ensemble import ExtraTreesRegressor # for building ExtraTree Regressor model\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b8a8b29",
   "metadata": {},
   "source": [
    "Next, we will use k-fold cross-validation to make out-of-fold predictions that will be used as the dataset to train the meta-model or “super learner.”\n",
    "\n",
    "This involves first splitting the data into k folds; we will use 10. For each fold, we will fit the base-model on the training part of the split and make out-of-fold predictions on the test part of the split. This is repeated for each base-model and all out-of-fold predictions are stored.\n",
    "\n",
    "Each out-of-fold prediction will be a column for the meta-model input. We will collect columns from each algorithm for one fold of the data, horizontally stacking the rows. Then for all groups of columns we collect, we will vertically stack these rows into one long dataset with 500 rows and nine columns.\n",
    "\n",
    "The get_out_of_fold_predictions() function below does this for a given test dataset and list of models; it will return the input and output dataset required to train the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c3a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(np.hstack(fold_yhats))\n",
    "\treturn np.vstack(meta_X), np.asarray(meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6e3e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (500, 9) (500,)\n"
     ]
    }
   ],
   "source": [
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d36a8efa",
   "metadata": {},
   "source": [
    "Next, we can fit all of the base-models on the entire training dataset.\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "Then, we can fit the meta-model on the prepared dataset.\n",
    "\n",
    "In this case, we will use a linear regression model as the meta-model, as was used in the original paper.\n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LinearRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "Next, we can evaluate the base-models on the holdout dataset.\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
    "And, finally, use the super learner (base and meta-model) to make predictions on the holdout dataset and evaluate the performance of the approach.\n",
    "\n",
    "The super_learner_predictions() function below will use the meta-model to make predictions for new data.\n",
    "\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "We can call this function and evaluate the results.\n",
    "\n",
    "...\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))\n",
    "Tying this all together, the complete example of a super learner algorithm for regression using scikit-learn models is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "538d08e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n",
      "X [[-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]\n",
      " [-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]]\n",
      "y [ 169.71507584  -56.30693223  -14.94494743  -33.41319138  -83.00205931\n",
      "   49.24785885  -13.51454226  -92.23879588 -164.04440221]\n",
      "===================get out of fold=========================\n",
      "Shape of X and y (500, 100) (500,)\n",
      "X [[-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]\n",
      " [-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]]\n",
      "y [ 169.71507584  -56.30693223  -14.94494743  -33.41319138  -83.00205931\n",
      "   49.24785885  -13.51454226  -92.23879588 -164.04440221]\n",
      "Meta  (500, 9) (500,)\n",
      "===================fit base models=========================\n",
      "Shape of X and y (500, 100) (500,)\n",
      "X [[-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]\n",
      " [-1.45412587  0.08008638  1.08519412 -0.11204679  0.29468594  0.84492951\n",
      "  -1.00811116  1.62160639 -0.20228665 -0.69050357 -1.21407836 -1.43345206\n",
      "  -1.35024259  0.84127711  0.91664021 -1.47066424  1.44626011 -0.73186133\n",
      "   0.01483951 -0.4863752  -1.78476461  0.94430145 -0.21079608 -0.28556668\n",
      "   0.43884436  1.05015894 -0.43188818  1.02402773 -0.43862051 -0.23308659\n",
      "   0.12886743  0.30232832 -0.75358659  1.61284737  1.46670653 -1.13486625\n",
      "  -0.2023176  -1.35092154  1.10786344 -0.72280319 -0.69040814 -0.68819222\n",
      "   0.49421152  2.32401083  0.18345993 -0.4445231   2.49940396 -0.273991\n",
      "  -0.2918462   0.52700002 -2.70403663  0.35069929  0.23660702  2.02505685\n",
      "   0.95695629 -0.3640541  -0.74261116 -0.73875395  0.52848976  0.05788144\n",
      "   0.50223642  0.07417785 -0.11082769 -0.44532347  0.02634377  0.72540289\n",
      "  -0.80219861  0.12737085 -0.24921959  0.84343955 -0.93032247  0.79961265\n",
      "   0.05713767  0.12553157  1.03636252 -0.03898067  0.93443905  1.24113271\n",
      "  -0.16104402  0.43475813 -0.65468372 -0.22750324 -0.87197797  1.3657439\n",
      "  -0.36071421 -0.3737311   0.03759677 -0.50335389  0.14798214  0.67282637\n",
      "  -0.77825823 -0.89297377  1.03772656 -1.69059621  0.4154662  -0.19779722\n",
      "  -0.3309068   0.24813676  1.31350413 -0.93511224]]\n",
      "y [ 169.71507584  -56.30693223  -14.94494743  -33.41319138  -83.00205931\n",
      "   49.24785885  -13.51454226  -92.23879588 -164.04440221]\n",
      "=====================fit the meta models=======================\n",
      "Shape of meta_X and meta_y (500, 9) (500,)\n",
      "meta_X [[-287.03012987 -164.10483967    1.27142804 -205.21362632 -164.892235\n",
      "  -186.30444669 -157.62271856 -130.8105946  -215.92299689]\n",
      " [-287.03012987 -164.10483967    1.27142804 -205.21362632 -164.892235\n",
      "  -186.30444669 -157.62271856 -130.8105946  -215.92299689]]\n",
      "meta_y [-287.05430259  287.85707261   53.09853347  244.75749521  -97.17401956\n",
      "  194.94945624  125.9259792   140.64880603  214.86676459]\n",
      "=====================evaluate base models=======================\n",
      "Shape of X_val and y_val (500, 100) (500,)\n",
      "X_val [[-0.97557146  0.93969758  0.16213572 -0.52913297 -1.01984587 -0.9901373\n",
      "   1.56466073 -0.36744062 -0.46690663  1.82074498 -0.6467878   0.61479769\n",
      "  -0.89039494 -1.80031842 -1.78301861  1.73235927  0.77342345 -1.33397366\n",
      "  -0.9791408   0.07862576 -0.08831843  0.84544925 -0.35788211  0.98591339\n",
      "  -1.00334012 -1.65008527 -2.52111713 -1.31012907 -0.57555725  0.51812877\n",
      "   0.2698705   0.071215   -1.08366099  1.86441656  1.95673744  1.11061634\n",
      "  -0.90255663  0.4218596  -0.27772365  0.7281598   0.40222432  0.49518658\n",
      "   0.91302656  1.65578871 -1.03973627 -0.55891755  2.02901532 -2.7783048\n",
      "   1.37248885  1.05469582 -0.45798678  1.5531764  -0.93604658  0.66476581\n",
      "  -0.16916835  1.39850481  1.11993537  1.01535053  0.48039914 -0.44960035\n",
      "  -0.37327802 -0.74254358  1.11134997  0.55633257  0.0827359  -0.23742269\n",
      "  -0.74593247  0.51321334 -0.95200952 -1.90626468  2.37210593  0.58418531\n",
      "   0.79254752 -0.71316908  2.15898927 -0.914939   -1.00542206  2.52406605\n",
      "   0.0668377  -0.46250974  0.80840922  0.42214629  0.12334939  1.53472102\n",
      "  -0.7029915   0.47428283  1.28024026 -0.80064709 -0.25966242 -0.36376614\n",
      "   0.94266754  0.72370919 -1.79953094 -0.37561839  0.49698054 -0.80550572\n",
      "   0.13015256 -0.91476075  0.82430932  1.07911797]\n",
      " [-0.97557146  0.93969758  0.16213572 -0.52913297 -1.01984587 -0.9901373\n",
      "   1.56466073 -0.36744062 -0.46690663  1.82074498 -0.6467878   0.61479769\n",
      "  -0.89039494 -1.80031842 -1.78301861  1.73235927  0.77342345 -1.33397366\n",
      "  -0.9791408   0.07862576 -0.08831843  0.84544925 -0.35788211  0.98591339\n",
      "  -1.00334012 -1.65008527 -2.52111713 -1.31012907 -0.57555725  0.51812877\n",
      "   0.2698705   0.071215   -1.08366099  1.86441656  1.95673744  1.11061634\n",
      "  -0.90255663  0.4218596  -0.27772365  0.7281598   0.40222432  0.49518658\n",
      "   0.91302656  1.65578871 -1.03973627 -0.55891755  2.02901532 -2.7783048\n",
      "   1.37248885  1.05469582 -0.45798678  1.5531764  -0.93604658  0.66476581\n",
      "  -0.16916835  1.39850481  1.11993537  1.01535053  0.48039914 -0.44960035\n",
      "  -0.37327802 -0.74254358  1.11134997  0.55633257  0.0827359  -0.23742269\n",
      "  -0.74593247  0.51321334 -0.95200952 -1.90626468  2.37210593  0.58418531\n",
      "   0.79254752 -0.71316908  2.15898927 -0.914939   -1.00542206  2.52406605\n",
      "   0.0668377  -0.46250974  0.80840922  0.42214629  0.12334939  1.53472102\n",
      "  -0.7029915   0.47428283  1.28024026 -0.80064709 -0.25966242 -0.36376614\n",
      "   0.94266754  0.72370919 -1.79953094 -0.37561839  0.49698054 -0.80550572\n",
      "   0.13015256 -0.91476075  0.82430932  1.07911797]]\n",
      "y_val [ 107.19699598  370.38402924  194.52857719   -2.78429032   -7.42954964\n",
      " -205.86535176  -29.7371163   175.63073621 -103.84705264]\n",
      "LinearRegression: RMSE 0.585\n",
      "ElasticNet: RMSE 64.719\n",
      "SVR: RMSE 180.717\n",
      "DecisionTreeRegressor: RMSE 144.815\n",
      "KNeighborsRegressor: RMSE 148.834\n",
      "AdaBoostRegressor: RMSE 87.222\n",
      "BaggingRegressor: RMSE 100.314\n",
      "RandomForestRegressor: RMSE 103.027\n",
      "ExtraTreesRegressor: RMSE 95.548\n",
      "=====================evaluate meta models=======================\n",
      "Shape of X_val (500, 100)\n",
      "X_val [[-0.97557146  0.93969758  0.16213572 -0.52913297 -1.01984587 -0.9901373\n",
      "   1.56466073 -0.36744062 -0.46690663  1.82074498 -0.6467878   0.61479769\n",
      "  -0.89039494 -1.80031842 -1.78301861  1.73235927  0.77342345 -1.33397366\n",
      "  -0.9791408   0.07862576 -0.08831843  0.84544925 -0.35788211  0.98591339\n",
      "  -1.00334012 -1.65008527 -2.52111713 -1.31012907 -0.57555725  0.51812877\n",
      "   0.2698705   0.071215   -1.08366099  1.86441656  1.95673744  1.11061634\n",
      "  -0.90255663  0.4218596  -0.27772365  0.7281598   0.40222432  0.49518658\n",
      "   0.91302656  1.65578871 -1.03973627 -0.55891755  2.02901532 -2.7783048\n",
      "   1.37248885  1.05469582 -0.45798678  1.5531764  -0.93604658  0.66476581\n",
      "  -0.16916835  1.39850481  1.11993537  1.01535053  0.48039914 -0.44960035\n",
      "  -0.37327802 -0.74254358  1.11134997  0.55633257  0.0827359  -0.23742269\n",
      "  -0.74593247  0.51321334 -0.95200952 -1.90626468  2.37210593  0.58418531\n",
      "   0.79254752 -0.71316908  2.15898927 -0.914939   -1.00542206  2.52406605\n",
      "   0.0668377  -0.46250974  0.80840922  0.42214629  0.12334939  1.53472102\n",
      "  -0.7029915   0.47428283  1.28024026 -0.80064709 -0.25966242 -0.36376614\n",
      "   0.94266754  0.72370919 -1.79953094 -0.37561839  0.49698054 -0.80550572\n",
      "   0.13015256 -0.91476075  0.82430932  1.07911797]\n",
      " [-0.97557146  0.93969758  0.16213572 -0.52913297 -1.01984587 -0.9901373\n",
      "   1.56466073 -0.36744062 -0.46690663  1.82074498 -0.6467878   0.61479769\n",
      "  -0.89039494 -1.80031842 -1.78301861  1.73235927  0.77342345 -1.33397366\n",
      "  -0.9791408   0.07862576 -0.08831843  0.84544925 -0.35788211  0.98591339\n",
      "  -1.00334012 -1.65008527 -2.52111713 -1.31012907 -0.57555725  0.51812877\n",
      "   0.2698705   0.071215   -1.08366099  1.86441656  1.95673744  1.11061634\n",
      "  -0.90255663  0.4218596  -0.27772365  0.7281598   0.40222432  0.49518658\n",
      "   0.91302656  1.65578871 -1.03973627 -0.55891755  2.02901532 -2.7783048\n",
      "   1.37248885  1.05469582 -0.45798678  1.5531764  -0.93604658  0.66476581\n",
      "  -0.16916835  1.39850481  1.11993537  1.01535053  0.48039914 -0.44960035\n",
      "  -0.37327802 -0.74254358  1.11134997  0.55633257  0.0827359  -0.23742269\n",
      "  -0.74593247  0.51321334 -0.95200952 -1.90626468  2.37210593  0.58418531\n",
      "   0.79254752 -0.71316908  2.15898927 -0.914939   -1.00542206  2.52406605\n",
      "   0.0668377  -0.46250974  0.80840922  0.42214629  0.12334939  1.53472102\n",
      "  -0.7029915   0.47428283  1.28024026 -0.80064709 -0.25966242 -0.36376614\n",
      "   0.94266754  0.72370919 -1.79953094 -0.37561839  0.49698054 -0.80550572\n",
      "   0.13015256 -0.91476075  0.82430932  1.07911797]]\n",
      "Super Learner: RMSE 0.587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of a super learner model for regression\n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    " \n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    " \n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    " \n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LinearRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    " \n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
    " \n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    " \n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "#print('X and y', X[[1,1]], y[1])\n",
    "print('X', X[[1,1]])\n",
    "print('y', y[1:10])\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "print('===================get out of fold=========================')\n",
    "print('Shape of X and y', X.shape, y.shape)\n",
    "#print('X and y', X[[1,1]], y[1])\n",
    "print('X', X[[1,1]])\n",
    "print('y', y[1:10])\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "print('===================fit base models=========================')\n",
    "print('Shape of X and y', X.shape, y.shape)\n",
    "#print('X and y', X[[1,1]], y[1])\n",
    "print('X', X[[1,1]])\n",
    "print('y', y[1:10])\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "print('=====================fit the meta models=======================')\n",
    "print('Shape of meta_X and meta_y', meta_X.shape, meta_y.shape)\n",
    "print('meta_X', meta_X[[1,1]])\n",
    "print('meta_y', meta_y[1:10])\n",
    "# evaluate base models\n",
    "print('=====================evaluate base models=======================')\n",
    "print('Shape of X_val and y_val', X_val.shape, y_val.shape)\n",
    "#print('X_val and y_val', X[[1,1]], y[1])\n",
    "print('X_val', X_val[[1,1]])\n",
    "print('y_val', y_val[1:10])\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "print('=====================evaluate meta models=======================')\n",
    "print('Shape of X_val', X_val.shape)\n",
    "print('X_val', X_val[[1,1]])\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890d586",
   "metadata": {},
   "source": [
    "# Build Super Learner Regression Model with mlens library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a06276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlens\n",
      "  Downloading mlens-0.2.3-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from mlens) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from mlens) (1.6.2)\n",
      "Installing collected packages: mlens\n",
      "Successfully installed mlens-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd598b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n",
      "                                  score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  adaboostregressor          93.37    14.18  0.79  0.02  0.05  0.01\n",
      "layer-1  baggingregressor          107.92    13.90  0.32  0.03  0.01  0.00\n",
      "layer-1  decisiontreeregressor     152.68    16.43  0.04  0.00  0.00  0.00\n",
      "layer-1  elasticnet                 61.29    10.28  0.01  0.00  0.00  0.00\n",
      "layer-1  extratreesregressor       104.00    16.33  0.19  0.02  0.02  0.01\n",
      "layer-1  kneighborsregressor       144.17    18.33  0.00  0.00  0.16  0.06\n",
      "layer-1  linearregression            0.61     0.05  0.02  0.01  0.00  0.00\n",
      "layer-1  randomforestregressor     110.37    14.50  0.28  0.03  0.00  0.00\n",
      "layer-1  svr                       161.13    24.01  0.02  0.00  0.01  0.00\n",
      "\n",
      "Super Learner: RMSE 0.540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of a super learner for regression using the mlens library\n",
    "from math import sqrt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    " \n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LinearRegression())\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    " \n",
    "# cost function for base models\n",
    "def rmse(yreal, yhat):\n",
    "\treturn sqrt(mean_squared_error(yreal, yhat))\n",
    " \n",
    "# create the super learner\n",
    "def get_super_learner(X):\n",
    "\tensemble = SuperLearner(scorer=rmse, folds=10, shuffle=True, sample_size=len(X))\n",
    "\t# add base models\n",
    "\tmodels = get_models()\n",
    "\tensemble.add(models)\n",
    "\t# add the meta model\n",
    "\tensemble.add_meta(LinearRegression())\n",
    "\treturn ensemble\n",
    " \n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# create the super learner\n",
    "ensemble = get_super_learner(X)\n",
    "# fit the super learner\n",
    "ensemble.fit(X, y)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# evaluate meta model\n",
    "yhat = ensemble.predict(X_val)\n",
    "print('Super Learner: RMSE %.3f' % (rmse(y_val, yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6a914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a84af6",
   "metadata": {},
   "source": [
    "# Build Super Learner Classification Model with mlens library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc0dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (500, 100) (500,) Test (500, 100) (500,)\n",
      "                                   score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  adaboostclassifier           0.91     0.03  0.71  0.02  0.07  0.01\n",
      "layer-1  baggingclassifier            0.85     0.07  0.27  0.01  0.01  0.00\n",
      "layer-1  decisiontreeclassifier       0.71     0.08  0.04  0.00  0.00  0.00\n",
      "layer-1  extratreesclassifier         0.84     0.07  0.11  0.02  0.01  0.00\n",
      "layer-1  gaussiannb                   0.98     0.02  0.01  0.00  0.00  0.00\n",
      "layer-1  kneighborsclassifier         0.95     0.04  0.00  0.00  0.20  0.02\n",
      "layer-1  logisticregression           0.97     0.03  0.01  0.00  0.00  0.00\n",
      "layer-1  randomforestclassifier       0.83     0.04  0.19  0.03  0.01  0.00\n",
      "layer-1  svc                          0.98     0.02  0.09  0.01  0.01  0.01\n",
      "\n",
      "Super Learner: 98.800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of a super learner using the mlens library\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    " \n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    " \n",
    "# create the super learner\n",
    "def get_super_learner(X):\n",
    "\tensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X))\n",
    "\t# add base models\n",
    "\tmodels = get_models()\n",
    "\tensemble.add(models)\n",
    "\t# add the meta model\n",
    "\tensemble.add_meta(LogisticRegression(solver='lbfgs'))\n",
    "\treturn ensemble\n",
    " \n",
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# create the super learner\n",
    "ensemble = get_super_learner(X)\n",
    "# fit the super learner\n",
    "ensemble.fit(X, y)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# make predictions on hold out set\n",
    "yhat = ensemble.predict(X_val)\n",
    "print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095fe44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ea590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051114a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c79e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b21c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
