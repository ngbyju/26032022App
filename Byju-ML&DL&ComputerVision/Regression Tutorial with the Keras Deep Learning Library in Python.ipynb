{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d71a47",
   "metadata": {},
   "source": [
    "# Regression Tutorial with the Keras Deep Learning Library in Python\n",
    "https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98095c45",
   "metadata": {},
   "source": [
    "Keras is a deep learning library that wraps the efficient numerical libraries Theano and TensorFlow.\n",
    "\n",
    "In this post you will discover how to develop and evaluate neural network models using Keras for a regression problem.\n",
    "\n",
    "After completing this step-by-step tutorial, you will know:\n",
    "\n",
    "How to load a CSV dataset and make it available to Keras.\n",
    "How to create a neural network model with Keras for a regression problem.\n",
    "How to use scikit-learn with Keras to evaluate models using cross-validation.\n",
    "How to perform data preparation in order to improve skill with Keras models.\n",
    "How to tune the network topology of models with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e94fe",
   "metadata": {},
   "source": [
    "# 1. Problem Description"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c9d5138",
   "metadata": {},
   "source": [
    "\n",
    "The problem that we will look at in this tutorial is the Boston house price dataset.\n",
    "\n",
    "You can download this dataset and save it to your current working directly with the file name housing.csv (update: download data from here).\n",
    "\n",
    "The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include things like crime rate, proportion of nonretail business acres, chemical concentrations and more.\n",
    "\n",
    "This is a well-studied problem in machine learning. It is convenient to work with because all of the input and output attributes are numerical and there are 506 instances to work with.\n",
    "\n",
    "Reasonable performance for models evaluated using Mean Squared Error (MSE) are around 20 in squared thousands of dollars (or $4,500 if you take the square root). This is a nice target to aim for with our neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e59f6",
   "metadata": {},
   "source": [
    "# 2. Develop a Baseline Neural Network Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcff21dd",
   "metadata": {},
   "source": [
    "\n",
    "In this section we will create a baseline neural network model for the regression problem.\n",
    "\n",
    "Letâ€™s start off by including all of the functions and objects we will need for this tutorial.\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "...\n",
    "We can now load our dataset from a file in the local directory.\n",
    "\n",
    "The dataset is in fact not in CSV format in the UCI Machine Learning Repository, the attributes are instead separated by whitespace. We can load this easily using the pandas library. We can then split the input (X) and output (Y) attributes so that they are easier to model with Keras and scikit-learn.\n",
    "\n",
    "...\n",
    "# load dataset\n",
    "dataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "We can create Keras models and evaluate them with scikit-learn by using handy wrapper objects provided by the Keras library. This is desirable, because scikit-learn excels at evaluating models and will allow us to use powerful data preparation and model evaluation schemes with very few lines of code.\n",
    "\n",
    "The Keras wrappers require a function as an argument. This function that we must define is responsible for creating the neural network model to be evaluated.\n",
    "\n",
    "Below we define the function to create the baseline model to be evaluated. It is a simple model that has a single fully connected hidden layer with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform.\n",
    "\n",
    "The efficient ADAM optimization algorithm is used and a mean squared error loss function is optimized. This will be the same metric that we will use to evaluate the performance of the model. It is a desirable metric because by taking the square root gives us an error value we can directly understand in the context of the problem (thousands of dollars).\n",
    "\n",
    "If you are new to Keras or deep learning, see this Keras tutorial.\n",
    "\n",
    "...\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "The Keras wrapper object for use in scikit-learn as a regression estimator is called KerasRegressor. We create an instance and pass it both the name of the function to create the neural network model as well as some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults.\n",
    "\n",
    "The final step is to evaluate this baseline model. We will use 10-fold cross validation to evaluate the model.\n",
    "\n",
    "...\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "Tying this all together, the complete example is listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc5162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras[tensorflow]\n",
      "  Downloading scikeras-0.9.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikeras[tensorflow]) (1.1.1)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikeras[tensorflow]) (20.9)\n",
      "Requirement already satisfied: tensorflow>=2.7.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikeras[tensorflow]) (2.7.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from packaging>=0.21->scikeras[tensorflow]) (2.4.7)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.20.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.42.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.19.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.22.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.4.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (12.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\byju\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.7.0->scikeras[tensorflow]) (3.1.1)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8c8223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: -30.04 (27.08) MSE\n"
     ]
    }
   ],
   "source": [
    "# Regression Example With Boston Dataset: Baseline\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(model=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d901854f",
   "metadata": {},
   "source": [
    "Running this code gives us an estimate of the modelâ€™s performance on the problem for unseen data.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Note: The mean squared error is negative because scikit-learn inverts so that the metric is maximized instead of minimized. You can ignore the sign of the result.\n",
    "\n",
    "The result reports the mean squared error including the average and standard deviation (average variance) across all 10 folds of the cross validation evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796d589",
   "metadata": {},
   "source": [
    "# 3. Modeling The Standardized Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54c88bf4",
   "metadata": {},
   "source": [
    "\n",
    "An important concern with the Boston house price dataset is that the input attributes all vary in their scales because they measure different quantities.\n",
    "\n",
    "It is almost always good practice to prepare your data before modeling it using a neural network model.\n",
    "\n",
    "Continuing on from the above baseline model, we can re-evaluate the same model using a standardized version of the input dataset.\n",
    "\n",
    "We can use scikit-learnâ€™s Pipeline framework to perform the standardization during the model evaluation process, within each fold of the cross validation. This ensures that there is no data leakage from each testset cross validation fold into the training data.\n",
    "\n",
    "The code below creates a scikit-learn Pipeline that first standardizes the dataset then creates and evaluate the baseline neural network model.\n",
    "\n",
    "...\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "Tying this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ab5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: -29.15 (25.71) MSE\n"
     ]
    }
   ],
   "source": [
    "# Regression Example With Boston Dataset: Standardized\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97f63787",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example provides an improved performance over the baseline model without standardized data, dropping the error.\n",
    "\n",
    "Standardized: -29.54 (27.87) MSE\n",
    "A further extension of this section would be to similarly apply a rescaling to the output variable such as normalizing it to the range of 0-1 and use a Sigmoid or similar activation function on the output layer to narrow output predictions to the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fc613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Standardized input and normalised output: -546.51 (276.41) MSE\n"
     ]
    }
   ],
   "source": [
    "# Regression Example With Boston Dataset: Standardized input and mormalised output\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"For Standardized input and normalised output: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e23f42",
   "metadata": {},
   "source": [
    "# 4. Tune The Neural Network Topology"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd05fad7",
   "metadata": {},
   "source": [
    "There are many concerns that can be optimized for a neural network model.\n",
    "\n",
    "Perhaps the point of biggest leverage is the structure of the network itself, including the number of layers and the number of neurons in each layer.\n",
    "\n",
    "In this section we will evaluate two additional network topologies in an effort to further improve the performance of the model. We will look at both a deeper and a wider network topology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfedf3",
   "metadata": {},
   "source": [
    "# 4.1. Evaluate a Deeper Network Topology"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee96b434",
   "metadata": {},
   "source": [
    "One way to improve the performance a neural network is to add more layers. This might allow the model to extract and recombine higher order features embedded in the data.\n",
    "\n",
    "In this section we will evaluate the effect of adding one more hidden layer to the model. This is as easy as defining a new function that will create this deeper model, copied from our baseline model above. We can then insert a new line after the first hidden layer. In this case with about half the number of neurons.\n",
    "\n",
    "...\n",
    "# define the model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "Our network topology now looks like:\n",
    "\n",
    "13 inputs -> [13 -> 6] -> 1 output\n",
    "We can evaluate this network topology in the same way as above, whilst also using the standardization of the dataset that above was shown to improve performance.\n",
    "\n",
    "...\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "Tying this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0388e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger (Deeper Network): -23.75 (28.16) MSE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression Example With Boston Dataset: Standardized and Larger\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define the model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Larger (Deeper Network): %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abbd8ef2",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "For the model with just standardized inputs, running this model does show a further improvement in performance from 29 down to 24 thousand squared dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd92e2c",
   "metadata": {},
   "source": [
    "# 4.2. Evaluate a Wider Network Topology"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53244956",
   "metadata": {},
   "source": [
    "\n",
    "Another approach to increasing the representational capability of the model is to create a wider network.\n",
    "\n",
    "In this section we evaluate the effect of keeping a shallow network architecture and nearly doubling the number of neurons in the one hidden layer.\n",
    "\n",
    "Again, all we need to do is define a new function that creates our neural network model. Here, we have increased the number of neurons in the hidden layer compared to the baseline model from 13 to 20.\n",
    "\n",
    "...\n",
    "# define wider model\n",
    "def wider_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "Our network topology now looks like:\n",
    "\n",
    "13 inputs -> [20] -> 1 output\n",
    "We can evaluate the wider network topology using the same scheme as above:\n",
    "\n",
    "...\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "Tying this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df010b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider Network: -20.83 (23.35) MSE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define wider model\n",
    "def wider_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Wider Network: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff6708d3",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Building the model does see a further drop in error to about 21 thousand squared dollars. This is not a bad result for this problem.\n",
    "\n",
    "Wider: -21.71 (24.39) MSE\n",
    "It would have been hard to guess that a wider network would outperform a deeper network on this problem. The results demonstrate the importance of empirical testing when it comes to developing neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b5873",
   "metadata": {},
   "source": [
    "# What if we develop a deeper and wider network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8074ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper and Wider Network: -21.99 (25.52) MSE\n"
     ]
    }
   ],
   "source": [
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "# define wider model\n",
    "def wider_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu')) # we added a second hidden layer with 6 neurons\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(model=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"Deeper and Wider Network: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f382543",
   "metadata": {},
   "source": [
    "Tried multiple configurations of architecture with deeper and wider network. It did not help much. Might be a case of overfitting. There might be a better optimal solution with deeper and wider network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
